{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ACD_Process.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08a4988fbe744812b0aec29a56927f68":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_54b31fa1db4d4a94b2d485d590582ff9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f7df7edff9d9421e9b188d2d3f49843b","IPY_MODEL_b9ce59b412ba44db9a62443ae149be3b"]}},"54b31fa1db4d4a94b2d485d590582ff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7df7edff9d9421e9b188d2d3f49843b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8937ae0b527e4d1183e34cc5a2618a5d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":512,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":512,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0ee06b0511d543338fcd4926b993af93"}},"b9ce59b412ba44db9a62443ae149be3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6b8096596bb6474aaee9b9b2f902a6d0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 512/512 [00:00&lt;00:00, 3.40kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2af44925097d44afb022e12baed89311"}},"8937ae0b527e4d1183e34cc5a2618a5d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0ee06b0511d543338fcd4926b993af93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b8096596bb6474aaee9b9b2f902a6d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2af44925097d44afb022e12baed89311":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc69b7e308de4519b39f388f5d7349d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0b882f24a5bf476c924fb8129731f98f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_74a686ca4baf448684595f39230c17ac","IPY_MODEL_e4ef84429a3a4a5894f9a2b5ba225811"]}},"0b882f24a5bf476c924fb8129731f98f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"74a686ca4baf448684595f39230c17ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ce60b41d64f94ac692fe361dda865509","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1115590446,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1115590446,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c51801d501844e90b5c1f27d0af2030d"}},"e4ef84429a3a4a5894f9a2b5ba225811":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_837ace3696ce48cdba8fb9d51bfd57fc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.12G/1.12G [00:23&lt;00:00, 47.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a00005563f94b6db064c2dea396fd99"}},"ce60b41d64f94ac692fe361dda865509":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c51801d501844e90b5c1f27d0af2030d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"837ace3696ce48cdba8fb9d51bfd57fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7a00005563f94b6db064c2dea396fd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"91726d00b78a425cad7197ff7443f8c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fd19ff87455f4c618cc0c0eb3e69b448","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_285e83eed3dd49cc9674fcb98a51454d","IPY_MODEL_0a8fd5a1eff74e8bac36be8930d3a3b6"]}},"fd19ff87455f4c618cc0c0eb3e69b448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"285e83eed3dd49cc9674fcb98a51454d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_11444ea1a3cc4bf4bf7245eafcb0ca5b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee5d5d06e80040088640f3a440089ee3"}},"0a8fd5a1eff74e8bac36be8930d3a3b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3e45ffce5ebb49ac959a74ce0ea08cf5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.07M/5.07M [00:00&lt;00:00, 15.4MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9b9d6eaf2f04230b37d9cfa38b0571b"}},"11444ea1a3cc4bf4bf7245eafcb0ca5b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ee5d5d06e80040088640f3a440089ee3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e45ffce5ebb49ac959a74ce0ea08cf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c9b9d6eaf2f04230b37d9cfa38b0571b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f65a1be7671423f824058d7daf9e019":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2d78c9f01b1946049992c54e9406fa19","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3952cc1699824d1da52ed5d68f9acec0","IPY_MODEL_80963d039d50426faad3d428e4ebe595"]}},"2d78c9f01b1946049992c54e9406fa19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3952cc1699824d1da52ed5d68f9acec0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5ebcb8675d7b4e12a7da73309c3b08f8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":9096718,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9096718,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65ea25a08d1e49aa86236726f158ab89"}},"80963d039d50426faad3d428e4ebe595":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b94c56c63294d778f25c52a97bd71e2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9.10M/9.10M [00:12&lt;00:00, 703kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_945af86fb6e0470e9da2dd3c712b15f3"}},"5ebcb8675d7b4e12a7da73309c3b08f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"65ea25a08d1e49aa86236726f158ab89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b94c56c63294d778f25c52a97bd71e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"945af86fb6e0470e9da2dd3c712b15f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5cff0b7173ab4767856b9d18f967621e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e9a8e9243a2e42bf94bcd81f49d1d1be","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_da6de0458b8b46fd97db4779f4348d92","IPY_MODEL_186ac397bd764e92beeb6282f496ce74"]}},"e9a8e9243a2e42bf94bcd81f49d1d1be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da6de0458b8b46fd97db4779f4348d92":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a1acd417f2ff41f78a68d5f243f4e25c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bb62c5551b1143b281e544cfaab43323"}},"186ac397bd764e92beeb6282f496ce74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_46b4fb6d603a4181a0b9c73fa4cc8163","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:01&lt;00:00, 527B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef617c284b0a4ff2b033dd51878a27e5"}},"a1acd417f2ff41f78a68d5f243f4e25c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bb62c5551b1143b281e544cfaab43323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46b4fb6d603a4181a0b9c73fa4cc8163":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef617c284b0a4ff2b033dd51878a27e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ccc38a6eced84613b20d5f1854708818":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_27f3fbb23b494f71802782abe0c6b1b2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4c24dd06cea34ff28c25d4de2a5c9755","IPY_MODEL_7c04665e425945739c824ecf60d2f672"]}},"27f3fbb23b494f71802782abe0c6b1b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4c24dd06cea34ff28c25d4de2a5c9755":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5aa70cc105b54bc5b19ae8cd916a3c9f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_104c116b06fa4a7487f8e1afd297cf83"}},"7c04665e425945739c824ecf60d2f672":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7e13276f183a40a4910cfa2873f3e73d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:43&lt;00:00, 16.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15e08a0c22c943eaabde6ec1fa1f6ea0"}},"5aa70cc105b54bc5b19ae8cd916a3c9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"104c116b06fa4a7487f8e1afd297cf83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e13276f183a40a4910cfa2873f3e73d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"15e08a0c22c943eaabde6ec1fa1f6ea0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73fc091bc6f84b07a75fac05b5d7c79a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_91edbef317b948e0ab03ac42a5ba1971","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ff9f2c1e18574a9ba73c346010e8c2a5","IPY_MODEL_6f15262557bf4c00b3167d40a334b4e3"]}},"91edbef317b948e0ab03ac42a5ba1971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff9f2c1e18574a9ba73c346010e8c2a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1c1856b96d1b4671bd929cde3da59e7c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_268778f492df43a4a144fbdd1b5a2d4b"}},"6f15262557bf4c00b3167d40a334b4e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1475f8b3d13541e493c148c030b91caf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:02&lt;00:00, 12.4B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e7f690b1c384e42b0aaa476f3d8eedd"}},"1c1856b96d1b4671bd929cde3da59e7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"268778f492df43a4a144fbdd1b5a2d4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1475f8b3d13541e493c148c030b91caf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e7f690b1c384e42b0aaa476f3d8eedd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cacf895ff5f54cc38427cce05a4594dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6199a348c51249d5802ffc9efc6e5394","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dbd04a9f11994ed09dc83f3a8f77f910","IPY_MODEL_fc73f3ac69b54f1e9bb00598a8025555"]}},"6199a348c51249d5802ffc9efc6e5394":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dbd04a9f11994ed09dc83f3a8f77f910":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d2b324d6651242d0a6346ebe86c7c4ef","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5aa33b7b34c42a784f3f84172e952c9"}},"fc73f3ac69b54f1e9bb00598a8025555":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7cb582465ed64fb6aaf5d82dbf20aa37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:14&lt;00:00, 67.7kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1707f69fcdab4883a154ff9835a414a2"}},"d2b324d6651242d0a6346ebe86c7c4ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a5aa33b7b34c42a784f3f84172e952c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7cb582465ed64fb6aaf5d82dbf20aa37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1707f69fcdab4883a154ff9835a414a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"decb1c4b29bc44469b62077032299965":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e86db00a69384d578ea1e0dd9ebf9d13","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_184c58cb84534fa79ec0d91fc91c9056","IPY_MODEL_0926716c2edf4ab58da026bc226b906b"]}},"e86db00a69384d578ea1e0dd9ebf9d13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"184c58cb84534fa79ec0d91fc91c9056":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9082ca1fdc1645d891b317ee4978aeeb","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1961828,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1961828,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3b3bde3356fe44e4afa3b088b35560de"}},"0926716c2edf4ab58da026bc226b906b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8bade341d1d64088a95487f2346efa1a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.96M/1.96M [00:12&lt;00:00, 154kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4b05e976c1fe4e51b30092da3db5afe7"}},"9082ca1fdc1645d891b317ee4978aeeb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3b3bde3356fe44e4afa3b088b35560de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bade341d1d64088a95487f2346efa1a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4b05e976c1fe4e51b30092da3db5afe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18398673eed042d5b272405cd79a0fd9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_79b3283d6ea4419aa612714c878d3836","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2bb850be8e934f848c5864240bb410fb","IPY_MODEL_dd185c07aa4c414cabffb4cc827bcc62"]}},"79b3283d6ea4419aa612714c878d3836":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2bb850be8e934f848c5864240bb410fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d58da5a815ae4d7db92c23cb3baff1f8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1403,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1403,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ab2b7684d4ea4d7eb17b724d862009b4"}},"dd185c07aa4c414cabffb4cc827bcc62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0484e91741914360a26dd20669d268a7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.40k/1.40k [00:00&lt;00:00, 2.65kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a8ed1d12d1b4dfdb7e6f0617e6c7845"}},"d58da5a815ae4d7db92c23cb3baff1f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ab2b7684d4ea4d7eb17b724d862009b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0484e91741914360a26dd20669d268a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5a8ed1d12d1b4dfdb7e6f0617e6c7845":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"862b404ff3714df79b0cf2f6476289d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c6c8a8439a0a40c08c6fa1bc979ac650","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_876af751a4bb44bb92c3ef5acb8e9f6e","IPY_MODEL_f0133707a3ec45cc8d7a1347234a9337"]}},"c6c8a8439a0a40c08c6fa1bc979ac650":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"876af751a4bb44bb92c3ef5acb8e9f6e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9cc7d3e6aade4e498a2ce546e2bcaaf5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":581243664,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":581243664,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1709ada5746640f7a5305dcc9892183b"}},"f0133707a3ec45cc8d7a1347234a9337":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_38d4224ed2404ee490e7013c09520c3c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 581M/581M [00:17&lt;00:00, 32.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c077c82df644d0694bc6e503c655246"}},"9cc7d3e6aade4e498a2ce546e2bcaaf5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1709ada5746640f7a5305dcc9892183b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38d4224ed2404ee490e7013c09520c3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c077c82df644d0694bc6e503c655246":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f44ff5a254ab49349bfb3333ea820ef0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8178bb41753e45d290dacb2c39e13e38","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7b925bb64fa14f46ac86a8d8bacf6291","IPY_MODEL_eda2051f8b094a838daba7d87623b433"]}},"8178bb41753e45d290dacb2c39e13e38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b925bb64fa14f46ac86a8d8bacf6291":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fff17351f209481998f8b2dc1db78d7f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":254703,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":254703,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d982a80e86447a2a3fd68e161fcc400"}},"eda2051f8b094a838daba7d87623b433":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f15504c3fae546dfb5459a59dc7a9224","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 255k/255k [00:00&lt;00:00, 1.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9026066054894123afb1308a69d12300"}},"fff17351f209481998f8b2dc1db78d7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1d982a80e86447a2a3fd68e161fcc400":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f15504c3fae546dfb5459a59dc7a9224":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9026066054894123afb1308a69d12300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e71f6a2651ac4b8f814e613d899e398c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9123738699634839a6b86da7efa9c838","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4b128a874a31440d93ae527446873b5f","IPY_MODEL_478681fcec4846c1bbc83a2c605946d3"]}},"9123738699634839a6b86da7efa9c838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b128a874a31440d93ae527446873b5f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3df6926711134a9c814e576a69db3538","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":557,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":557,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a6bc4911c2246009638754ffa5b4dd4"}},"478681fcec4846c1bbc83a2c605946d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_988e0836b6144073a6b70e0086956024","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 557/557 [00:13&lt;00:00, 40.1B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd1f5ce37bac4347829681cdacbb57a3"}},"3df6926711134a9c814e576a69db3538":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5a6bc4911c2246009638754ffa5b4dd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"988e0836b6144073a6b70e0086956024":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fd1f5ce37bac4347829681cdacbb57a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b1a815a76b2442f98dcae52a0b90778":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4b04c6e8e2724af98bed2859828645d2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8c467f778d57410ba9d4b59ad3d72caf","IPY_MODEL_116ff0e774eb48a29b8409d034ee59eb"]}},"4b04c6e8e2724af98bed2859828645d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c467f778d57410ba9d4b59ad3d72caf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_17f4dadc198a47119e534390db7ff0e5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":542923308,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":542923308,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a88f4f2a44764010ab363e45e1b31150"}},"116ff0e774eb48a29b8409d034ee59eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cbab3b7b2ffc4bd6b6ac329f32dc9a1b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 543M/543M [00:13&lt;00:00, 39.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6da4953feaad4e55afda481d1e6c3067"}},"17f4dadc198a47119e534390db7ff0e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a88f4f2a44764010ab363e45e1b31150":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cbab3b7b2ffc4bd6b6ac329f32dc9a1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6da4953feaad4e55afda481d1e6c3067":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"893c1d3ac1bd4056ab575cf2bb9fe4f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f4fa37c4d7aa4d658e5460842e9e845f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9f35012bca2f4b028690836045fe19ec","IPY_MODEL_834a71cb8bd14b51b9e50a62ba5b42c8"]}},"f4fa37c4d7aa4d658e5460842e9e845f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f35012bca2f4b028690836045fe19ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0329581fdffe436d800c367c4faae514","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":895321,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":895321,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97e8a482f35543a885c567b60c6f43b8"}},"834a71cb8bd14b51b9e50a62ba5b42c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8a70b664047540feaa80f6342161e0b2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 895k/895k [00:00&lt;00:00, 3.54MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_460c487a6e2b42c581622255a54e62fb"}},"0329581fdffe436d800c367c4faae514":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"97e8a482f35543a885c567b60c6f43b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a70b664047540feaa80f6342161e0b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"460c487a6e2b42c581622255a54e62fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5495960939ef42c7ae914b713a9e7879":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0fe12bce58494e8e985acd9c28bda824","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dcfb3ae2ff294fd1b65977670f9c279f","IPY_MODEL_46cc127894a44ae0ae7f7ff975ed7119"]}},"0fe12bce58494e8e985acd9c28bda824":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcfb3ae2ff294fd1b65977670f9c279f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9611d20ab242454e9956f431f7bcd78c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1135173,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1135173,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1731f43f3f5545aaaae4844b7f24fd3a"}},"46cc127894a44ae0ae7f7ff975ed7119":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4ca5209b711d40c590003747e36314cd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.14M/1.14M [00:00&lt;00:00, 6.80MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9a23f34dbaa4c949ea2a4a01d021b6b"}},"9611d20ab242454e9956f431f7bcd78c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1731f43f3f5545aaaae4844b7f24fd3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ca5209b711d40c590003747e36314cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f9a23f34dbaa4c949ea2a4a01d021b6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"Y1tj4Qe9I7pd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625625050485,"user_tz":-420,"elapsed":9977,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"92b36ac1-8912-4ba4-c846-cb1a67ba2043"},"source":["!pip install scikit-multilearn\n","!pip install transformers\n","!pip install emoji"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVsBJDUMs5_W","executionInfo":{"status":"ok","timestamp":1625625050487,"user_tz":-420,"elapsed":38,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"9c926d4d-720a-4462-f6fb-952499dfdcda"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bHW4hGIHI9Wp"},"source":["# Thư viện"]},{"cell_type":"code","metadata":{"id":"exk1LKxTIfJj","executionInfo":{"status":"ok","timestamp":1625625050491,"user_tz":-420,"elapsed":37,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}}},"source":["from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_multilabel_classification\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from gensim.models import KeyedVectors\n","import pandas as pd\n","import numpy as np\n","from sklearn.naive_bayes import ComplementNB\n","import seaborn as sb\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","import joblib\n","import nltk\n","from sklearn.multioutput import MultiOutputClassifier\n","\n","from nltk.tokenize import TweetTokenizer\n","from bs4 import BeautifulSoup\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.naive_bayes import GaussianNB\n","from skmultilearn.problem_transform import BinaryRelevance\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, Conv1D, GlobalMaxPool1D, Dense, Flatten, Dropout, GRU, Bidirectional\n","\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","import re\n","import seaborn as sns\n","import emoji\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from keras.models import load_model\n","from joblib import dump\n","import torch\n","import pandas as pd\n","import numpy as np\n","\n","# Thu vien transformer cho Classification\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n","\n","# Xu ly label\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Metric danh gia \n","from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n","\n","# Ve do thi\n","import seaborn as sn\n","import matplotlib.pyplot as plt"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ZSMSY1g1Txv"},"source":["# Đọc dữ liệu"]},{"cell_type":"code","metadata":{"id":"wq7Z-Nvt1Txw","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1625625050492,"user_tz":-420,"elapsed":37,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"8afddda7-6cef-4ca5-a539-bcf816a7bcf3"},"source":["df_train = pd.read_csv('/content/drive/MyDrive/NLP for Data Science/NLP_Project/Data_csv/ACD_data/ACD_train.csv')\n","df_dev = pd.read_csv('/content/drive/MyDrive/NLP for Data Science/NLP_Project/Data_csv/ACD_data/ACD_dev.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/NLP for Data Science/NLP_Project/Data_csv/ACD_data/ACD_test.csv')\n","\n","df_train.head(10)"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>aspect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Giá 53k size vừa.</td>\n","      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Nhưng nói chung cũng hơi đắt.</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Mình ăn rất hôi mùi dầu.</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Mình ăn chưa baoh thấy mùi hôi hải sản.</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3 dĩa vs 2 lon Revive mà có 190k thui(.</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Ở đây ngay khu vắng nên khách cũng không đông ...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Quán đông lắm, gọi món phải đợi hơi lâu, ko bi...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Mình uống trà sữa lài hạnh nhân khá thơm nha, ...</td>\n","      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Không gian trang trí đơn giàn mà ấm cúng, nên ...</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Chè lạnh thì hình như lúc nào cũng là sương sa...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             comment                                aspect\n","0                                  Giá 53k size vừa.  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                      Nhưng nói chung cũng hơi đắt.  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","2                           Mình ăn rất hôi mùi dầu.  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","3            Mình ăn chưa baoh thấy mùi hôi hải sản.  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","4            3 dĩa vs 2 lon Revive mà có 190k thui(.  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","5  Ở đây ngay khu vắng nên khách cũng không đông ...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n","6  Quán đông lắm, gọi món phải đợi hơi lâu, ko bi...  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n","7  Mình uống trà sữa lài hạnh nhân khá thơm nha, ...  [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n","8  Không gian trang trí đơn giàn mà ấm cúng, nên ...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","9  Chè lạnh thì hình như lúc nào cũng là sương sa...  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"Dzi4rBBBLPGF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625625050494,"user_tz":-420,"elapsed":24,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"cf91bd55-6659-4ac4-e304-132d86eef001"},"source":["len(df_train), len(df_dev), len(df_test)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7028, 771, 1938)"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"Op_Dpr7uaF90"},"source":["# Hàm tiền xử lý"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kmxq67xEaEu1","executionInfo":{"status":"ok","timestamp":1625625061174,"user_tz":-420,"elapsed":10699,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"1543a310-c258-4ef6-c797-77bb032b2682"},"source":["# Xóa code html\n","def remove_html(txt):\n","    return re.sub(r'<[^>]*>', '', txt)\n","\n","# xóa link\n","def remove_url(document):\n","    document = re.sub(r\"(http\\S+)|(www\\S+)\", '', document)\n","    return document\n","\n","# xóa tag user\n","def remove_tag_user(document):\n","    document = re.sub(r\"(@\\w{1,15})\", '', document)\n","    return document\n","\n","# xóa hashtags\n","def remove_hashtags(document):\n","    document = re.sub(\"#(\\w{1,})\", '', document)\n","    return document\n","\n","# Chuẩn hóa unicode\n","import regex as re\n","uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n","unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n","def loaddicchar():\n","    dic = {}\n","    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n","        '|')\n","    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n","        '|')\n","    for i in range(len(char1252)):\n","        dic[char1252[i]] = charutf8[i]\n","    return dic\n","dicchar = loaddicchar()\n","def convert_unicode(txt):\n","    return re.sub(\n","        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n","        lambda x: dicchar[x.group()], txt)\n","\n","# Chuẩn hóa dấu tiếng việt\n","bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n","                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n","                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n","                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n","                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n","                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n","                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n","                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n","                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n","                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n","                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n","                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n","bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n","nguyen_am_to_ids = {}\n","for i in range(len(bang_nguyen_am)):\n","    for j in range(len(bang_nguyen_am[i]) - 1):\n","        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n","def chuan_hoa_dau_tu_tieng_viet(word):\n","    if not is_valid_vietnam_word(word):\n","        return word\n","    chars = list(word)\n","    dau_cau = 0\n","    nguyen_am_index = []\n","    qu_or_gi = False\n","    for index, char in enumerate(chars):\n","        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n","        if x == -1:\n","            continue\n","        elif x == 9: \n","            if index != 0 and chars[index - 1] == 'q':\n","                chars[index] = 'u'\n","                qu_or_gi = True\n","        elif x == 5: \n","            if index != 0 and chars[index - 1] == 'g':\n","                chars[index] = 'i'\n","                qu_or_gi = True\n","        if y != 0:\n","            dau_cau = y\n","            chars[index] = bang_nguyen_am[x][0]\n","        if not qu_or_gi or index != 1:\n","            nguyen_am_index.append(index)\n","    if len(nguyen_am_index) < 2:\n","        if qu_or_gi:\n","            if len(chars) == 2:\n","                x, y = nguyen_am_to_ids.get(chars[1])\n","                chars[1] = bang_nguyen_am[x][dau_cau]\n","            else:\n","                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n","                if x != -1:\n","                    chars[2] = bang_nguyen_am[x][dau_cau]\n","                else:\n","                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n","            return ''.join(chars)\n","        return word\n","    for index in nguyen_am_index:\n","        x, y = nguyen_am_to_ids[chars[index]]\n","        if x == 4 or x == 8:  \n","            chars[index] = bang_nguyen_am[x][dau_cau]\n","            return ''.join(chars)\n","    if len(nguyen_am_index) == 2:\n","        if nguyen_am_index[-1] == len(chars) - 1:\n","            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n","            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n","        else:\n","            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n","            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n","    else:\n","        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n","        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n","    return ''.join(chars)\n","def is_valid_vietnam_word(word):\n","    chars = list(word)\n","    nguyen_am_index = -1\n","    for index, char in enumerate(chars):\n","        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n","        if x != -1:\n","            if nguyen_am_index == -1:\n","                nguyen_am_index = index\n","            else:\n","                if index - nguyen_am_index != 1:\n","                    return False\n","                nguyen_am_index = index\n","    return True\n","def chuan_hoa_dau_cau_tieng_viet(sentence):\n","    sentence = sentence.lower()\n","    words = sentence.split()\n","    for index, word in enumerate(words):\n","        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n","        if len(cw) == 3:\n","            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n","        words[index] = ''.join(cw)\n","    return ' '.join(words)\n","\n","# VNCoreNLP tách từ\n","!pip install deplacy vncorenlp\n","!test -d VnCoreNLP || git clone --depth=1 https://github.com/vncorenlp/VnCoreNLP\n","from vncorenlp import VnCoreNLP\n","vncorenlp = VnCoreNLP(\"VnCoreNLP/VnCoreNLP-1.1.1.jar\",annotators=\"wseg\")\n","def vncore_tokenize(document):\n","    l = vncorenlp.tokenize(document)[0]\n","    document = ''\n","    for i in range(len(l)):\n","        document += l[i] + ' '\n","    document = document[:-1]\n","    return document\n","\n","# Stopword\n","def stopword(path):\n","    stopwords = []\n","    f = open(path, 'r')\n","    for line in f:\n","        line = line.rstrip()\n","        line = line.replace(' ', '_')\n","        stopwords.append(line)\n","    f.close()\n","    return stopwords\n","path = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/vietnamese-stopwords.txt'\n","stopwords = stopword(path)\n","def remove_stopword(text):\n","    text_new = ''\n","    list_word = text.split()\n","    for word in list_word:\n","        if word not in stopwords:\n","            text_new += word + ' '\n","    text_new = text_new[:-1]\n","    return text_new\n","\n","# Chuẩn hóa từ về dạng chuẩn (ngonnnn thành ngon)\n","def standardize_word(text):\n","    new_text = []\n","    for i in range(len(text)):\n","        if i == 0 or (text[i-1] != text[i]):\n","            new_text.append(text[i])\n","    new_text = \"\".join(new_text)\n","    return new_text\n","\n","# Chuyển emotion, emoji thành word form\n","def load_emoticon_to_word_dict(file_path):\n","    the_dict = pd.read_excel(file_path).to_dict()\n","    emoticon_to_word = dict((v,key) for key,value in the_dict.items() for v in value.values() if type(v) is str)\n","    return emoticon_to_word\n","def load_word_dict_to_vietnameses(file_path):\n","    eng_to_vie_df = pd.read_excel(file_path)\n","    eng_to_vie = dict(zip(eng_to_vie_df.Emojize, eng_to_vie_df.Translated))\n","    return eng_to_vie\n","emoticon_to_word = load_emoticon_to_word_dict(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/emoticon-emoji.xlsx\")\n","word_to_vietnamese = load_word_dict_to_vietnameses(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Emoji_to_VN.xlsx\")\n","punct_to_remove = \"\"\"(<[/:.,;`~-'\"@#*+=\\]>)\"\"\"\n","def emo_to_word_form(text, emoticon_to_word, punct_to_remove):\n","    #Change emoji to word form\n","    new_text = emoji.demojize(text)\n","    #Change emoticon to word form\n","    word_list = [emoticon_to_word[k] if k in emoticon_to_word.keys() else k for k in new_text.split(\" \")] \n","    new_text = \" \".join(word_list)\n","    #Add space between punctuations such as ([:.,!?()])\n","    new_text = re.sub('([:.,!?()])', r' \\1 ', new_text) \n","    #Remove some punctuation\n","    text_remove_punct = [new_text[i] if new_text[i] not in punct_to_remove else '' for i in range(len(new_text))]\n","    new_text = \"\".join(text_remove_punct)\n","    #Remove extra space\n","    new_text = re.sub(' +', ' ', new_text)\n","    new_text = re.sub(r'\\s+', ' ', new_text).strip()\n","    return new_text\n","\n","# Chuyển word form sang tiếng việt\n","def work_form_to_vietnamese(text, word_to_vietnamese):\n","    word_list_word_to_viet = []\n","    for word in text.split(\" \"):\n","        if word in word_to_vietnamese.keys():\n","            word = word_to_vietnamese[word]\n","        if '_' in word:\n","            continue\n","        word_list_word_to_viet.append(word)\n","    new_text = \" \".join(word_list_word_to_viet)\n","    return new_text\n","\n","# Chuyển 100k thành giá tiền\n","def money(text):\n","    text = re.sub('(\\d+k)|(\\d+ k)|(\\d+đồng)|(\\d+ đồng)|(\\d+ngàn)|(\\d+ ngàn)|(\\d+nghìn)|(\\d+ nghìn)', 'giá tiền', text)\n","    return text"],"execution_count":51,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: deplacy in /usr/local/lib/python3.7/dist-packages (1.9.9)\n","Requirement already satisfied: vncorenlp in /usr/local/lib/python3.7/dist-packages (1.0.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ETK6mL4BaPK9","executionInfo":{"status":"ok","timestamp":1625625061188,"user_tz":-420,"elapsed":38,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}}},"source":["# Preprocessing final\n","def text_preprocess(document):\n","    # chuẩn hóa unicode\n","    document = convert_unicode(document)\n","    # chuẩn hóa cách gõ dấu tiếng Việt\n","    document = chuan_hoa_dau_cau_tieng_viet(document)\n","\n","    # xóa html code\n","    document = remove_html(document)\n","    # xóa link\n","    document =  remove_url(document)\n","    # xóa tag user\n","    document = remove_tag_user(document)\n","    # xóa hashtags\n","    document =  remove_hashtags(document)\n","\n","    # Đưa từ về dạng chuẩn\n","    document = standardize_word(document)\n","    # Chuyển emotion, emoji thành word form\n","    document = emo_to_word_form(document, emoticon_to_word, punct_to_remove)\n","    # Chuyển word form sang Vietnamese.\n","    document = work_form_to_vietnamese(document, word_to_vietnamese)\n","    # Chuẩn hóa giá tiền\n","    document = money(document)\n","    # tách từ\n","    document = vncore_tokenize(document)\n","    # # đưa về lower\n","    document = document.lower()\n","    # # xóa stopword\n","    document = remove_stopword(document)\n","    # # xóa các ký tự không cần thiết\n","    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', document)\n","    # # xóa khoảng trắng thừa\n","    document = re.sub(r'\\s+', ' ', document).strip()\n","\n","    return document"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":195},"id":"UoFRH4qFaRLX","executionInfo":{"status":"ok","timestamp":1625625111906,"user_tz":-420,"elapsed":50747,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"31a87398-2fdf-4787-865c-6f9c4e0da0a6"},"source":["# Tiền xử lý comment\n","def preprocessing_comment(df):\n","    for i in range(len(df)):\n","        comment = df['comment'][i]\n","        df['comment'][i] = text_preprocess(comment)\n","    return df\n","\n","df_train = preprocessing_comment(df_train)\n","df_dev = preprocessing_comment(df_dev)\n","df_test = preprocessing_comment(df_test)\n","\n","df_train.head()"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>aspect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>giá giá tiền size</td>\n","      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hơi đắt</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hôi mùi dầu</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>baoh mùi hôi hải_sản</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3 dĩa vs 2 lon revive giá tiền thui</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               comment                                aspect\n","0                    giá giá tiền size  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                              hơi đắt  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","2                          hôi mùi dầu  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","3                 baoh mùi hôi hải_sản  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","4  3 dĩa vs 2 lon revive giá tiền thui  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"pZ6Xsgk81Txx","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1625625111908,"user_tz":-420,"elapsed":14,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"a1195b9f-8dea-40bc-bcfb-4a188f5ff4b3"},"source":["\n"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment</th>\n","      <th>aspect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>giá giá tiền size</td>\n","      <td>[0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hơi đắt</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hôi mùi dầu</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>baoh mùi hôi hải_sản</td>\n","      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3 dĩa vs 2 lon revive giá tiền thui</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               comment                                aspect\n","0                    giá giá tiền size  [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n","1                              hơi đắt  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n","2                          hôi mùi dầu  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","3                 baoh mùi hôi hải_sản  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","4  3 dĩa vs 2 lon revive giá tiền thui  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"W2q4-FwI1Txy"},"source":["# X và y"]},{"cell_type":"code","metadata":{"id":"_MZaohpk1Txy","executionInfo":{"status":"ok","timestamp":1625625111911,"user_tz":-420,"elapsed":14,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}}},"source":["X_train = df_train['comment'].values\n","X_dev = df_dev['comment'].values\n","X_test = df_test['comment'].values\n","\n","y_train = df_train['aspect'].values\n","y_dev = df_dev['aspect'].values\n","y_test = df_test['aspect'].values\n","\n","y_train = np.array(list(y_train), dtype=np.float)\n","y_dev = np.array(list(y_dev), dtype=np.float)\n","y_test = np.array(list(y_test), dtype=np.float)"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kyR3lOv1Txz","colab":{"base_uri":"https://localhost:8080/","height":648},"executionInfo":{"status":"ok","timestamp":1625625113758,"user_tz":-420,"elapsed":1860,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"389a9f5a-1283-4a43-dc6f-eaef12e10445"},"source":["# Độ dài cmt\n","def plot_number_word(X, name):\n","    l = []\n","    for i in range(len(X)):\n","        l.append(len(X[i].split()))\n","    plt.figure(figsize = (15, 2))\n","    sns.countplot(l)\n","    plt.title(\"Number of words in comment in \" + name + ' dataset.')\n","    plt.xlabel(\"Number of words\")\n","    plt.ylabel(\"Frequency\")\n","    plt.show()\n","\n","plot_number_word(X_train, 'train')\n","plot_number_word(X_dev, 'dev')\n","plot_number_word(X_test, 'test')"],"execution_count":59,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n","  FutureWarning\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA3sAAACqCAYAAAAUR1b0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcVZnH8e/PsCMQMBEhCQQBUXQEIWzDIoLKIrKGTRBUhHFBwZFRFAdwwQE3llFRBNmFQNgiRAEVAjqyhLAFIhIhmIQAkX2RJfLOH+c03DRV1dVVp5Luyu/zPP30Xd/71q06Vfete+4tRQRmZmZmZmbWXd60sBMwMzMzMzOz8lzsmZmZmZmZdSEXe2ZmZmZmZl3IxZ6ZmZmZmVkXcrFnZmZmZmbWhVzsmZmZmZmZdSEXe2ZmA4CksyR9ZyFtW5LOlPSkpFsWRg45j60lzWphvZ9J+u9O5NSNJN0jaeuFuP3fSDqwUKyF1m7MzAYDF3tmZjVImiHpMUnLVqZ9WtL1CzGtTtkC+BAwMiI2XtjJ9FdEfCYivr2w8xgIJB0r6bxGy0TEuyPi+hbjh6S1Wkru9e3vEBFntxOjFZKul/TpbtmOmVkzXOyZmdU3BDhsYSfRX5KG9HOV1YEZEfF8J/KpRdJiC2pbtuD4eTUzG1hc7JmZ1fd94AhJQ3vPkDQ6n+VYrDLttW/0JX1C0p8knSjpKUkPSPr3PH1mPmvYuyvbMEnXSnpW0iRJq1divzPPe0LSfZL2qsw7S9KpkiZKeh74QI18V5U0Ia8/XdLBefpBwOnAZpKek/TNGus+JGnDPLxfftzv7llf0uV5eElJJ0l6OP+dJGnJPG9rSbMkfVXSI8CZkpbOuT8p6V5go17b/aqk2Xl/3Cdp21pPUrUrX2U7X877eI6kT9ZaLy+/Uu7C+nDO4/LKvIPzvnoi77tVK/NC0uck3Z/z+7akNSX9n6RnJF0kaYleOX2lktOuknaU9Ncc/+uV2G+SdKSkv0l6PMdaKc/red0dKOnvkv4h6ag8b3vg68De+bm8s85jniHpg3n42Bz/nPw47pE0ps56N+TBO3P8ves8rytKulLS3LxPr5Q0shKndzv5o6Qf5GUflLRDg+frfZKm5FzHAUtV5tXdrqTjgC2BH+fcf5ynn6zUHp+RdJukLSvxNpY0Oc97VNKPKvM2zc/1U5LuVO4WW287ZmYLi4s9M7P6JgPXA0e0uP4mwF3AW4BfAReSCpq1gP1JB4Rvriy/H/BtYBhwB3A+gFJX0mtzjLcC+wA/lbRuZd2PAccBywF/rJHLhcAsYFVgLPBdSdtExBnAZ4A/R8SbI+KYGutOArbOw+8HHgC2qoxPysNHAZsC6wPrARsD36jEeRuwEulM4iHAMcCa+W874LXiV9I6wKHARhGxXJ4/o0ZutbwNWAEYARwE/ETSinWWPRdYBng3ad+emLe/DfA/wF7AKsBDpH1YtR2wYX7MXwFOIz2vo4D3APv2ymmpnNPRwC/yshuSioP/lrRGXvYLwK6kfbsq8CTwk17b3gJYB9gWOFrSuyLit8B3gXH5uVyv0U6q2Dk/tqHABKBmgRIRPc/5ejn+uMpjqz6vbwLOzOOrAf+sFzPbBLiP9Lr/HnCGJPVeKBfPl5Oes5WAi4E9KovU3W5EHAXcCByacz80r3Mr6fW6Eql9XSypp4A8GTg5IpYnvUYvynmMAK4CvpPXOwK4RNLwBtsxM1soXOyZmTV2NPAFScNbWPfBiDgzIv4FjCMVAd+KiJci4hrgZVLh1+OqiLghIl4iFU6bSRoF7ETqZnlmRMyLiNuBS4A9K+teERF/iohXI+LFahI5xubAVyPixYi4g3Q274AmH8ckUuEBqTD5n8p4tdjbLz++xyJiLvBN4OOVOK8Cx+TH/09SIXVcRDwRETOBUyrL/gtYElhX0uIRMSMi/tZkvq/kPF6JiInAc6TCaD6SVgF2AD4TEU/m5auP5ZcRMSU/H18jPR+jKyG+FxHPRMQ9wFTgmoh4ICKeBn4DvK9XTsdFxCukwmoYqZB4Nq9/L6lAhlR8HxURs/K2jwXGav4ukt+MiH9GxJ3AnZV1W/HHiJiYX6fnthBrvuc1Ih6PiEsi4oWIeJb0JcT7G6z/UET8Im//bFJxvXKN5TYFFgdOys/VeFKxBkAL2yUizsvrzYuIH5Jecz2vlVeAtSQNi4jnIuKmPH1/YGLeZ69GxLWkL4Z2bLiXzMwWAhd7ZmYNRMRU4ErgyBZWf7Qy/M8cr/e06pm9mZXtPgc8QTqzszqwSe4y9pSkp0jFyNtqrVvDqsAT+QC4x0Oks0zNmARsmYujIaQzHJvnwmcF0lnInu081Gsbq1bG5/YqRFftlfdr60bEdOBwUqHzmKQLq90o+/B4RMyrjL/A/Pu5xyjSfnmyxrz5Hkt+Ph5n/n3W+7ls9Nw+nouZnnm11u9ZfnXgsspzPY1U/FYLoEcqw/UeX7N6x1pK/bv2br7nVdIykn6u1P33GeAGYKjqX0v62vYj4oU8WOvxrArMjoioTHvtOWphu0g6QtI0SU/nfb0CqRCHdFb4HcBfJN0qaac8fXVgz17tcQtSkWpmNqC42DMz69sxwMHMf6DfczOTZSrTqsVXK0b1DOTunSsBD5MKokkRMbTy9+aI+Gxl3aC+h4GVJC1XmbYaMLuZpHLh9QKpe+ENEfEM6QD9ENJZoVcr21m9supqeVq9HOdQecx5+ep2fxURW+SYAZzQTL79MJO0X95wTSa9HkvuSvsWmtxnBfLaodfzvVRENLPtRq+DTum9zS+Tzo5tkrtA9nT/fEPXzH6aA4zo1cWz+prpa7vz5Zmvz/sK6QzzihExFHi6Z/mIuD8i9iV17z0BGJ9fBzOBc3s9P8tGxPG1tmNmtjC52DMz60MudsYBX6xMm0s68N9f0hBJnyJd19OOHSVtka9N+jZwU+7eeCXwDkkfl7R4/ttI0ruazH8m8H/A/0haStJ7SWctGt6iv5dJpGvoero5Xt9rHOAC4BuShksaRuoC22gbFwFfyzfWGEkqJoF0zZ6kbZRu8PIi6czXq3XitCQi5pC6W/4057C4pJ4C4QLgk5LWzzl8F7g5ImaUzKGOnwHHKd+gJ+/PXZpc91FgtKROfb4/Cry9j2WWIz1fTyndWKbWdaCt+DMwD/hifq52J10X2ux2e+e+XI43F1hM0tHA8j0zJe2fr8N7FXgqT36V9Jr+qKTtcttfSulGNT03oWlmH5mZLRAu9szMmvMtYNle0w4G/ovUve/dpIKqHb8iHaA+Qbpxx/4Aufvlh0k3ZnmYdFbtBNL1Rc3aFxid17+MdI3V7/qx/iTSwfENdcYh3bBiMummNHcDU/K0er5J6ob3IHAN6XqxHksCxwP/ID3et5Kumyvt46Rrs/4CPEbqOkreN/9NujZyDqmQ36cD26/lZNKNUq6R9CxwE+kmJs24OP9/XNKUDuR2LHB27r64V51lTgKWJj13NwG/LbHhiHgZ2B34BKmN7A1c2o/tnky69vFJSacAV+dl/kp6Hb7I/N2KtwfukfRcXneffE3iTGAX0p1P5+Z1/ovXj6l6b6fnh+z3a3snmJn1k+bv+m5mZmZmZmbdwGf2zMzMzMzMupCLPTMzMzMzsy7kYs/MzMzMzKwLdbTYk/SlfFHyVEkX5DtWrSHpZknTJY3Ld51D0pJ5fHqeP7qTuZmZmZmZmXWzjhV7kkaQblM+JiLeQ/oh3n1Id5A7MSLWAp4k3f6b/P/JPP1Eyv+ekpmZmZmZ2SJjsQUQf2lJr5B+eHgOsA3wsTz/bNJtnE8l3cb42Dx9PPBjSYoGtwsdNmxYjB49uiOJm5mZmZmZDXS33XbbPyJieK15HSv2ImK2pB8Afyf9yOk1wG3AUxExLy82CxiRh0eQf98mIuZJehp4C+n3cmoaPXo0kydP7tAjMDMzMzMzG9gkPVRvXie7ca5IOlu3BrAq6ceIty8Q9xBJkyVNnjt3brvhzMzMzMzMulInb9DyQeDBiJgbEa8AlwKbA0Ml9ZxRHAnMzsOzgVEAef4KwOO9g0bEaRExJiLGDB9e82ylmZmZmZnZIq+Txd7fgU0lLSNJwLbAvcB1wNi8zIHAFXl4Qh4nz/9Do+v1zMzMzMzMrL5OXrN3s6TxwBRgHnA7cBpwFXChpO/kaWfkVc4AzpU0HXiCdOdOG2Cm/nTnInHe87kJ843/+bSdisQF2OyQK4vFMjMzMzMbrDp6N86IOAY4ptfkB4CNayz7IrBnJ/MxMzMzMzNbVHT0R9XNzMzMzMxs4XCxZ2ZmZmZm1oVc7JmZmZmZmXWhjl6zZ9btrvjlDkXi7PKp3xSJY2ZmZmbWw2f2zMzMzMzMupDP7FnXu/b0HYvE+dCnJxaJY2ZmZma2IPjMnpmZmZmZWRdysWdmZmZmZtaFXOyZmZmZmZl1IRd7ZmZmZmZmXcjFnpmZmZmZWRdysWdmZmZmZtaFXOyZmZmZmZl1IRd7ZmZmZmZmXcjFnpmZmZmZWRdysWdmZmZmZtaFXOyZmZmZmZl1IRd7ZmZmZmZmXaijxZ6koZLGS/qLpGmSNpO0kqRrJd2f/6+Yl5WkUyRNl3SXpA06mZuZmZmZmVk36/SZvZOB30bEO4H1gGnAkcDvI2Jt4Pd5HGAHYO38dwhwaodzMzMzMzMz61odK/YkrQBsBZwBEBEvR8RTwC7A2Xmxs4Fd8/AuwDmR3AQMlbRKp/IzMzMzMzPrZot1MPYawFzgTEnrAbcBhwErR8ScvMwjwMp5eAQws7L+rDxtDtYvf//ffYvEWe0LFxSJY2ZmZmZmC14nu3EuBmwAnBoR7wOe5/UumwBERADRn6CSDpE0WdLkuXPnFkvWzMzMzMysmzRV7En6txZizwJmRcTNeXw8qfh7tKd7Zv7/WJ4/GxhVWX9knjafiDgtIsZExJjhw4e3kJaZmZmZmVn3a7Yb508lLQmcBZwfEU/3tUJEPCJppqR1IuI+YFvg3vx3IHB8/n9FXmUCcKikC4FNgKcr3T3NFim/Omu7InE+9omri8QxMzMzs8GnqWIvIraUtDbwKeA2SbcAZ0bEtX2s+gXgfElLAA8AnySdTbxI0kHAQ8BeedmJwI7AdOCFvKyZmZmZmZm1oOkbtETE/ZK+AUwGTgHeJ0nA1yPi0jrr3AGMqTFr2xrLBvD5ZvMxMzMzMzOz+pq9Zu+9kk4k/U7eNsBHI+JdefjEDuZnZmZmZmZmLWj2zN7/AqeTzuL9s2diRDycz/aZmZmZmZnZANJssfcR4J8R8S8ASW8CloqIFyLi3I5lZ2ZmZmZmZi1p9nf2fgcsXRlfJk8zMzMzMzOzAajZYm+piHiuZyQPL9OZlMzMzMzMzKxdzRZ7z0vaoGdE0obAPxssb2ZmZmZmZgtRs9fsHQ5cLOlhQMDbgL07lpWZmZmZmZm1pdkfVb9V0juBdfKk+yLilc6lZWZmZmZmZu1o+kfVgY2A0XmdDSQREed0JCszMzMzMzNrS1PFnqRzgTWBO4B/5ckBuNgzMzMzMzMbgJo9szcGWDciopPJmJmZmZmZWRnN3o1zKummLGZmZmZmZjYINHtmbxhwr6RbgJd6JkbEzh3JyszMzMzMzNrSbLF3bCeTMDMzMzMzs7Ka/emFSZJWB9aOiN9JWgYY0tnUzKwTfnHOdkXiHHzA1UXimJmZmVlnNHXNnqSDgfHAz/OkEcDlnUrKzMzMzMzM2tPsDVo+D2wOPAMQEfcDb+1UUmZmZmZmZtaeZou9lyLi5Z4RSYuRfmfPzMzMzMzMBqBmi71Jkr4OLC3pQ8DFwK+bWVHSEEm3S7oyj68h6WZJ0yWNk7REnr5kHp+e54/u/8MxMzMzMzMzaL7YOxKYC9wN/AcwEfhGk+seBkyrjJ8AnBgRawFPAgfl6QcBT+bpJ+blzMzMzMzMrAVNFXsR8WpE/CIi9oyIsXm4z26ckkYCHwFOz+MCtiHd7AXgbGDXPLxLHifP3zYvb2ZmZmZmZv3U1E8vSHqQGtfoRcTb+1j1JOArwHJ5/C3AUxExL4/PIt3Zk/x/Zo47T9LTefl/NJOjmZmZmZmZva7ZH1UfUxleCtgTWKnRCpJ2Ah6LiNskbd1aejXjHgIcArDaaquVCmtmBZz0qzK/4Xf4x/wbfmZmZmbtarYb5+OVv9kRcRKpe2YjmwM7S5oBXEjqvnkyMDTfzRNgJDA7D88GRsFrd/tcAXi8Ri6nRcSYiBgzfPjwZtI3MzMzMzNb5DT7o+obVP7GSPoMfZwVjIivRcTIiBgN7AP8ISL2A64DxubFDgSuyMMT8jh5/h+auS7QzMzMzMzM3qjZbpw/rAzPA2YAe7W4za8CF0r6DnA7cEaefgZwrqTpwBOkAtHMzMzMzMxa0FSxFxEfaGcjEXE9cH0efgDYuMYyL5KuBTQzMzMzM7M2NXs3zv9sND8iflQmHTMzMzMzMyuhP3fj3Ih0XR3AR4FbgPs7kdSi4pFTjy0S522fLRPHzMzMzMy6R7PF3khgg4h4FkDSscBVEbF/pxIzMzMzMzOz1jV1N05gZeDlyvjLeZqZmZmZmZkNQM2e2TsHuEXSZXl8V+DszqRkZmZmZmZm7Wr2bpzHSfoNsGWe9MmIuL1zaZmZmZmZmVk7mu3GCbAM8ExEnAzMkrRGh3IyMzMzMzOzNjX70wvHkO7IuQ5wJrA4cB6weedSMzNLvjVuuyJxjt776iJxzMzMzAaDZs/s7QbsDDwPEBEPA8t1KikzMzMzMzNrT7PF3ssREUAASFq2cymZmZmZmZlZu5ot9i6S9HNgqKSDgd8Bv+hcWmZmZmZmZtaOPq/ZkyRgHPBO4BnSdXtHR8S1Hc7NzMzMzMzMWtRnsRcRIWliRPwb4ALPzMzMzMxsEGi2G+cUSRt1NBMzMzMzMzMrpqmfXgA2AfaXNIN0R06RTvq9t1OJmZktCIdeun2ROD/e/bdF4piZmZmV0rDYk7RaRPwdKPMjV2ZmZmZmZrZA9HVm73Jgg4h4SNIlEbHHgkjKzMzMzMzM2tPXNXuqDL+9P4EljZJ0naR7Jd0j6bA8fSVJ10q6P/9fMU+XpFMkTZd0l6QN+vdQzMzMzMzMrEdfxV7UGW7GPODLEbEusCnweUnrAkcCv4+ItYHf53GAHYC1898hwKn93J6ZmZmZmZllfXXjXE/SM6QzfEvnYXj9Bi3L11sxIuYAc/Lws5KmASOAXYCt82JnA9cDX83Tz4mIAG6SNFTSKjmOmZmZmZmZ9UPDYi8ihpTYiKTRwPuAm4GVKwXcI8DKeXgEMLOy2qw8zcWemZmZmZlZPzX70wstk/Rm4BLg8Ih4Rnr9MsD8g+396h4q6RBSN09WW221kqmamRWzwxV7FYv1m10uKhbLzMzMFh3N/qh6SyQtTir0zo+IS/PkRyWtkuevAjyWp88GRlVWH5mnzSciTouIMRExZvjw4Z1L3szMzMzMbBDrWLGndArvDGBaRPyoMmsCcGAePhC4ojL9gHxXzk2Bp329npmZmZmZWWs62Y1zc+DjwN2S7sjTvg4cD1wk6SDgIaCnr9NEYEdgOvAC8MkO5mZmNmjtePlXisSZuOv3isQxMzOzgaljxV5E/JH5f6evatsaywfw+U7lY2ZmZmZmtijp6DV7ZmZmZmZmtnC42DMzMzMzM+tCLvbMzMzMzMy6kIs9MzMzMzOzLuRiz8zMzMzMrAt18qcXzMxskNnxsu8WiTNxt68XiWNmZmat85k9MzMzMzOzLuRiz8zMzMzMrAu52DMzMzMzM+tCLvbMzMzMzMy6kG/Q0oe5P/tpkTjDP/O5InHMzMzMzMya4WLPzMw67iOXnlIkzlW7f/GNsS85rUzsPQ4pEsfMzGygcDdOMzMzMzOzLuRiz8zMzMzMrAu5G6eZmVkdO11yTpE4V+5xQJE4ZmZm/eFiz8zMbAHbafy4InGuHLt3kThmZtad3I3TzMzMzMysC/nMnpmZWRf56PjLi8T59dhd5xvfZfxvi8S9Yuz2ReKYmVnfBtSZPUnbS7pP0nRJRy7sfMzMzMzMzAarAXNmT9IQ4CfAh4BZwK2SJkTEvQs3MzMzM+uk3S65oUicy/bY6g3Txl5yW5HY4/fYsEgcM7MFacAUe8DGwPSIeABA0oXALoCLPTMzMxtQ9r70/iJxxu2+dpE4Zma1DKRibwQwszI+C9ikmRXnnnpekQSGf3b/InHMzMzMWnXUZbOLxDlutxHzjZ966aNF4gJ8dveV5xsff8k/isQdu8ewN0y75oIysT+87/yx/3z23CJxNztw+BumTf15mX39nv9Yue+FzBpQRCzsHACQNBbYPiI+ncc/DmwSEYf2Wu4Q4JA8ug5wX5ObGAaUebfojtjOefDHHow5dzK2cx78sQdjzp2M7ZwHf+zBmHMnYzvnwR97MObcydgDJefVI+KN3zowsM7szQZGVcZH5mnziYjTgNP6G1zS5IgY03p63RXbOQ/+2IMx507Gds6DP/ZgzLmTsZ3z4I89GHPuZGznPPhjD8acOxl7MOQ8kO7GeSuwtqQ1JC0B7ANMWMg5mZmZmZmZDUoD5sxeRMyTdChwNTAE+GVE3LOQ0zIzMzMzMxuUBkyxBxARE4GJHQrf766fXR7bOQ/+2IMx507Gds6DP/ZgzLmTsZ3z4I89GHPuZGznPPhjD8acOxl7wOc8YG7QYmZmZmZmZuUMpGv2zMzMzMzMrJBFotiTtL2k+yRNl3Rkwbi/lPSYpKmlYua4oyRdJ+leSfdIOqxg7KUk3SLpzhz7m6Vi5/hDJN0u6crCcWdIulvSHZImF449VNJ4SX+RNE3SZgVirpNz7fl7RtLhJfLN8b+Un7+pki6QtFShuIflmPe0m2+t9iFpJUnXSro//1+xYOw9c96vSmr57lV1Yn8/vz7uknSZpKGF4n47x7xD0jWSVi2Vc2XelyWFpDf+eFVrOR8raXbltb1jyZwlfSHv63skfa9UbEnjKjnPkHRHobjrS7qp571J0sYFc15P0p/ze9+vJS3fQtyanycl2mKD2G21xQZxS7TDerHbbov1Ylfmt9QWG+TcdltslHO7bbFB3m21xQZx226LDWKXaIs1j7+Ubkx4s9Ix6jilmxSWiHtojtnS+38fsc9XOq6eqvTetXihuGfkaXcpHZe9uWDOZ0l6sPLaW79g7BsrcR+WdHl/Y+c48x1DS9pW0pQc94+S1molLhHR1X+km738DXg7sARwJ7BuodhbARsAUwvnvAqwQR5eDvhrwZwFvDkPLw7cDGxaMPf/BH4FXFl4n8wAhnXoNXI28Ok8vAQwtHD8IcAjpN9AKRFvBPAgsHQevwj4RIG47wGmAsuQruf9HbBWG/He0D6A7wFH5uEjgRMKxn4X6bc3rwfGFM77w8BiefiEVvKuE3f5yvAXgZ+VyjlPH0W66dVDrbSfOjkfCxxR4PVWK/YH8utuyTz+1pL7ozL/h8DRhXK+BtghD+8IXF9wf9wKvD8Pfwr4dgtxa36elGiLDWK31RYbxC3RDuvFbrst1oudx1tuiw1ybrstNojddltstD8qy/S7LTbIue222CB2ibZY8/iL9Bm+T57+M+CzheK+DxhNG8dPDWLvmOcJuKBgztV2+CPye1Sh2GcBY9tsL30eQwOXAAe0GH++Y+j8+ntXHv4ccFYrcReFM3sbA9Mj4oGIeBm4ENilROCIuAF4okSsXnHnRMSUPPwsMI10gF8idkTEc3l08fxX5MJNSSOBjwCnl4i3IEhagXSQdQZARLwcEU8V3sy2wN8i4qGCMRcDlpa0GKk4e7hAzHcBN0fECxExD5gE7N5qsDrtYxdScU3+v2up2BExLSLuayVeE7GvyfsE4CbS74CWiPtMZXRZWmyLDd6LTgS+0oG4basT+7PA8RHxUl7msYKxAZAkYC/SAUqJuAH0fMu/Ai22xTqx3wHckIevBfZoIW69z5O222K92O22xQZxS7TDerHbbot9fHa33BY7fExQL3bbbbGvvFttiw3itt0WG8Qu0RbrHX9tA4zP0/vdFuvFjYjbI2JGf/NsMvbEPC+AW+hnW2wQ9xl47bWxNK21l44d5/YVO5/x3Qbo95m9OsfQRT5fFoVibwQwszI+i0JvkguCpNGkb2duLhhzSO428RhwbUSUin0S6cPs1ULxqgK4RtJtkg4pGHcNYC5wZj51frqkZQvGh/Sbkf0+sKwnImYDPwD+DswBno6IawqEngpsKektkpYhfXM3qkDcqpUjYk4efgRYuXD8BeFTwG9KBZN0nKSZwH7A0QXj7gLMjog7S8WsODR3s/mlWuyKW8c7SK/BmyVNkrRRwdg9tgQejYj7C8U7HPh+fg5/AHytUFyAe3j9y8k9abM99vo8KdoWO/FZ1Ufcttth79gl22I1dsm2WGN/FGuLvWIXbYt1nse222KvuEXbYq/YRdpi7+MvUs+zpypfYrR0jNrB47qGsXP3zY8Dvy0VV9KZpPekdwL/Wzjn43J7OVHSkoVjQyrUf9/ry6Nm1TqG/jQwUdIs0n4+vpWcF4Vib9DKfZUvAQ5v8YVTU0T8KyLWJ30Ts7Gk97QbU9JOwGMRcVvbCda2RURsAOwAfF7SVoXiLkbqOnVqRLwPeJ7UpakIpb73OwMXF4y5IulDZw1gVWBZSfu3GzcippG6Rl1DeuO+A/hXu3EbbC8o9G3bgiLpKGAecH6pmBFxVESMyjEPLREzF+tfp2DxWHEqsCawPunLhh8WjL0YsBKpy81/ARflb3hL2peCX76QzoB8KT+HXyL3EijkU8DnJN1G6lL2cquBGn2etNsWO/VZVS9uiXZYK3aptliNnfMs0hZr5FysLdaIXawtNnh9tNUWa8Qt1hZrxC7SFnsff5EKmrZ14riuydg/BW6IiBtLxY2IT5KObaYBexfM+Wuk/b0R6bX91YKxe7T0mm5wDP0lYMeIGAmcSera2m+LQrE3m/m/gRmZpw1o+duSS4DzI+LSTmwjUnfF64DtC4TbHNhZ0gxSV9ltJJ1XIC7w2tmsnq4kl5HeJEuYBcyqfDMznlT8lbIDMCUiHi0Y84PAgxExNyJeAS4F/r1E4Ig4IyI2jIitgCdJ/cVLelTSKgD5f0vd9BYGSZ8AdgL2ywfHpYA/+D0AAAeySURBVJ1PC12D6liT9GXAnblNjgSmSHpbu4Ej4tH8Yfcq8AvKtUVI7fHS3FXmFtI3nC3dWKCW3O15d2BcqZjAgaQ2COlLnWL7IyL+EhEfjogNSQcQf2slTp3PkyJtsVOfVfXilmiHTeTcclusEbtIW6yVc6m2WGd/FGmLDZ7HttpinbhF2mKdfV2kLfaoHH9tBgzN+wPaPEYtfFzXMLakY4DhpOvMisXN0/5FOpZs6zOxGjtSF92I1DX5TNp8r66xP4blmFe1EK7WMfRVwHqV49NxtHistygUe7cCayvd7WgJUpe6CQs5p4byt2dnANMioqUqvkHs4cp3MJO0NPAh4C/txo2Ir0XEyIgYTdrHf4iIts82AUhaVtJyPcOki/SL3AE1Ih4BZkpaJ0/aFri3ROys9FkESN03N5W0TH6tbEv6Bqxtkt6a/69G+iD+VYm4FRNIH8jk/1cUjt8RkrYnda/YOSJeKBh37croLhRoiwARcXdEvDUiRuc2OYt004FH2o3dUyBku1GoLWaXk24MgaR3kG6Y9I+C8T8I/CUiZhWM+TDw/jy8DVCqe2i1Pb4J+Abp5g39jVHv86Ttttipz6p6cUu0wwax226LtWKXaIsNcm67LTZ4Dttui328Plpuiw3itt0WG+zrEm2x1vHXNFLBMDYv1u+22KnjukaxJX0a2A7YN3/ZUCLufcp3m8zPw86tPI4GOfd8uSVSd8tW2kujfT2WdGOVF/sbt9YxNOl9aIXc/uD110v/RRt3pRksf6Rrj/5K+ibmqIJxLyB1nXiF9AZ+UKG4W5C61NxF6kp3B+k0bonY7wVuz7Gn0sId6ZrYxtYUvBsn6U6qd+a/e0o+hzn++sDkvE8uB1YsFHdZ4HFghQ7s42+S3mCmAueS75hWIO6NpGL3TmDbNmO9oX0AbwF+T/oQ/h2wUsHYu+Xhl4BHgasLxp5Ouva3pz22cqe+WnEvyc/hXcCvSTeKKJJzr/kzaO1unLVyPhe4O+c8AVil4H5eAjgv75MpwDYl9wfpbmyfKfya3gK4LbeZm4ENC8Y+jPTZ9VfStRpqIW7Nz5MSbbFB7LbaYoO4Jdphvdhtt8V6sdttiw1ybrstNojddltstD/aaYsNcm67LTaIXaIt1jz+Ih3j3JJf3xfTz8/zBnG/mNvhPFIhfHrBnOeRjql79lF/76j6hrikE1B/yq/pqaQz7MsXzPkPldjnke+qWSJ2nnc96Qxiv1/TvbaxNa/fjXO3nPOdOf7bW4mpHMzMzMzMzMy6yKLQjdPMzMzMzGyR42LPzMzMzMysC7nYMzMzMzMz60Iu9szMzMzMzLqQiz0zMzMzM7Mu5GLPzMwGJEkh6YeV8SMkHVso9lmSxva9ZNvb2VPSNEnXdXpbeXvHSjpiQWzLzMwGPhd7ZmY2UL0E7C5p2MJOpErSYv1Y/CDg4Ij4QAfyUP6RZzMzs5r8IWFmZgPVPOA04Eu9Z/Q+Myfpufx/a0mTJF0h6QFJx0vaT9Itku6WtGYlzAclTZb0V0k75fWHSPq+pFsl3SXpPypxb5Q0Abi3Rj775vhTJZ2Qpx1N+pHmMyR9v9fyP5G0cx6+TNIv8/CnJB2Xh/8zx5sq6fA8bbSk+ySdQ/pR31GSjsqP4Y/AOpVtfFHSvflxXNjPfW9mZl2gP99OmpmZLWg/Ae6S9L1+rLMe8C7gCeAB4PSI2FjSYcAXgMPzcqOBjYE1geskrQUcADwdERtJWhL4k6Rr8vIbAO+JiAerG5O0KnACsCHwJHCNpF0j4luStgGOiIjJvXK8EdgSmACMAFbJ07cELpS0IfBJYBNAwM2SJuX4awMHRsRNebl9gPVJn+lTgNtyrCOBNSLiJUlD+7H/zMysS/jMnpmZDVgR8QxwDvDFfqx2a0TMiYiXgL8BPcXa3aQCr8dFEfFqRNxPKgrfCXwYOEDSHcDNwFtIxRXALb0LvWwj4PqImBsR84Dzga36yPFGYEtJ65LOFD4qaRVgM+D/SGcEL4uI5yPiOeBSUiEI8FBE3JSHt8zLvZD31YTKNu4Czpe0P+ksqZmZLWJc7JmZ2UB3Eunat2Ur0+aRP8PydWtLVOa9VBl+tTL+KvP3aIle2wnSWbQvRMT6+W+NiOgpFp9v61FUNxQxGxgKbA/cQCr+9gKei4hn+1i92Tw+QjozugFwaz+vNTQzsy7gYs/MzAa0iHgCuIhU8PWYQeo2CbAzsHgLofeU9KZ8Hd/bgfuAq4HPSlocQNI7JC3bKAhwC/B+ScMkDQH2BSY1sf2bSF1Ke4q9I/J/8v9dJS2Tt79bZV7VDXm5pSUtB3w05/0mYFREXAd8FVgBeHMTOZmZWRfxt3xmZjYY/BA4tDL+C+AKSXcCv6W1s25/JxVqywOfiYgXJZ1O6uo5RZKAucCujYJExBxJRwLXkc4MXhURVzSx/RuBD0fEdEkPASvlaUTEFEln5fwgXXd4u6TRvbY9RdI44E7gMeDWPGsIcJ6kFXJOp0TEU03kZGZmXUQRvXuxmJmZmZmZ2WDnbpxmZmZmZmZdyMWemZmZmZlZF3KxZ2ZmZmZm1oVc7JmZmZmZmXUhF3tmZmZmZmZdyMWemZmZmZlZF3KxZ2ZmZmZm1oVc7JmZmZmZmXWh/wfMAyiDgQbVEQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1080x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n","  FutureWarning\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1080x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n","  FutureWarning\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 1080x144 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"raY9jST81Txy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625625113760,"user_tz":-420,"elapsed":17,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"9cd77c2a-c84f-4ec0-81de-a944184e2d49"},"source":["X_train[:3]"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['giá giá tiền size', 'hơi đắt', 'hôi mùi dầu'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"Z2Rx70fR1Txz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625625113761,"user_tz":-420,"elapsed":14,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}},"outputId":"24599f25-ef3a-4415-aab4-01d93b54faa4"},"source":["y_train[:3]"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"4l5nwbDSE6E2"},"source":["# ================================================================"]},{"cell_type":"markdown","metadata":{"id":"huDfxe08QvS-"},"source":["# Process\n","fastText: 200000\n","* Have - 3888\n","* No - 257\n","\n","multi: 21591\n","* Have - 2126\n","* No - 2019\n","\n","wiki: 231486\n","* Have - 3395\n","* No - 750\n","\n","bpemb: 200000\n","* Have - 2684\n","* No - 1461"]},{"cell_type":"code","metadata":{"id":"IY6UTn5lXKC9","executionInfo":{"status":"ok","timestamp":1625625113762,"user_tz":-420,"elapsed":10,"user":{"displayName":"Hiếu Trần Trung","photoUrl":"","userId":"07195769082401599220"}}},"source":["model_name = [\"Native Bayes\",\n","              \"Logistic Regression\",\n","              \"SVM\",\n","              \"Text-CNN-fastText\",\n","              \"Text-CNN-Wikipedia Word2vec\",\n","              \"Text-CNN-BPEmb\",\n","              \"Text-CNN-MULTI_WC_F_E_B\",\n","              \"Bi-GRU-fastText\",\n","              \"Bi-GRU-Wikipedia Word2vec\",\n","              \"Bi-GRU-BPEmb\",\n","              \"Bi-GRU-MULTI_WC_F_E_B\",\n","              \"BERT XLM-R\",\n","              \"BERT MULTILINGUAL\"\n","              \"FPTAI (cased)\",\n","              \"PhoBERT\"]"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mlBZOPvx9nvY"},"source":["# Model---Naive Bayes"]},{"cell_type":"code","metadata":{"id":"R7b8_VKK9vU9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625019832082,"user_tz":-420,"elapsed":71459,"user":{"displayName":"Huy Lam Gia","photoUrl":"","userId":"16757804681341342086"}},"outputId":"e87ba385-5348-4ea6-cc1c-b95774283ea6"},"source":["tv = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3))\n","x_train = tv.fit_transform(X_train)\n","x_dev = tv.transform(X_dev)\n","x_test = tv.transform(X_test)\n","\n","model = BinaryRelevance(ComplementNB())\n","model.fit(x_train, y_train)\n","\n","y_train_pred = model.predict(x_train)\n","y_dev_pred = model.predict(x_dev)\n","y_test_pred = model.predict(x_test)\n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[0]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                    \"PRECISION-MICRO\": [precision_micro_test], \n","                    \"RECALL-MICRO\":[recall_micro_test],\n","                    \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.8261240751280592\n","Accuracy dev:  0.35149156939040205\n","Accuracy test:  0.3606811145510836\n","F1 macro test:  0.4514536954872778\n","Recall macro test:  0.3388594200198905\n","Precision macro test:  0.8622121704069586\n","F1 micro test:  0.5752541532358045\n","Recall micro test:  0.44123240775960443\n","Precision micro test:  0.8262108262108262\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l4URVO6L9_9e"},"source":["# Model---Logistic Regression"]},{"cell_type":"code","metadata":{"id":"G-NxdmvH-FhW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625019843067,"user_tz":-420,"elapsed":10998,"user":{"displayName":"Huy Lam Gia","photoUrl":"","userId":"16757804681341342086"}},"outputId":"e6bdfdb5-ae8c-4c59-e71f-adc9567530f2"},"source":["tv = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3))\n","x_train = tv.fit_transform(X_train)\n","x_dev = tv.transform(X_dev)\n","x_test = tv.transform(X_test)\n","\n","model = MultiOutputClassifier(LogisticRegression())\n","model.fit(x_train, y_train)\n","\n","y_train_pred = model.predict(x_train)\n","y_dev_pred = model.predict(x_dev)\n","y_test_pred = model.predict(x_test)\n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[1]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                    \"PRECISION-MICRO\": [precision_micro_test], \n","                    \"RECALL-MICRO\":[recall_micro_test],\n","                    \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.3948491747296528\n","Accuracy dev:  0.3942931258106355\n","Accuracy test:  0.3926728586171311\n","F1 macro test:  0.5241229187943804\n","Recall macro test:  0.41896215460850517\n","Precision macro test:  0.7855860265373713\n","F1 micro test:  0.6315302820157189\n","Recall micro test:  0.5195891974134652\n","Precision micro test:  0.8049499116087213\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7h0tMt5MQFpa"},"source":["# Model---SVM"]},{"cell_type":"code","metadata":{"id":"JEqSVn7AOrwD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625019986212,"user_tz":-420,"elapsed":143155,"user":{"displayName":"Huy Lam Gia","photoUrl":"","userId":"16757804681341342086"}},"outputId":"9f3721f0-a289-4981-fe14-1a1b03744cfe"},"source":["tv = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3))\n","x_train = tv.fit_transform(X_train)\n","x_dev = tv.transform(X_dev)\n","x_test = tv.transform(X_test)\n","\n","model = MultiOutputClassifier(SVC())\n","model.fit(x_train, y_train)\n","\n","y_train_pred = model.predict(x_train)\n","y_dev_pred = model.predict(x_dev)\n","y_test_pred = model.predict(x_test)\n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[2]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                    \"PRECISION-MICRO\": [precision_micro_test], \n","                    \"RECALL-MICRO\":[recall_micro_test],\n","                    \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.9147694934547524\n","Accuracy dev:  0.4409857328145266\n","Accuracy test:  0.4324045407636739\n","F1 macro test:  0.5545071679810701\n","Recall macro test:  0.4497144303015926\n","Precision macro test:  0.849304599637254\n","F1 micro test:  0.6578527467517666\n","Recall micro test:  0.5488779003423355\n","Precision micro test:  0.8208191126279863\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4CcyT56ukyRw"},"source":["# ========================================================"]},{"cell_type":"markdown","metadata":{"id":"6I6lurIjtDPZ"},"source":["# Model---Text-CNN---FastText"]},{"cell_type":"code","metadata":{"id":"lEUYmsjttJO7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625020155027,"user_tz":-420,"elapsed":168826,"user":{"displayName":"Huy Lam Gia","photoUrl":"","userId":"16757804681341342086"}},"outputId":"517392b8-293f-4226-bf6e-316d3e79867b"},"source":["x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","# Embedding matrix\n","def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  \n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype = np.float32)[:embedding_dim]\n","    return embedding_matrix\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","maxlen = 30\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 300\n","word_vector = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Word_Embedding/cc.vi.300.vec'\n","embedding_matrix = create_embedding_matrix(word_vector, tokenizer.word_index, embedding_dim)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Conv1D(128, 3, activation = 'relu'))\n","model.add(GlobalMaxPool1D())\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics= ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[3]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                    \"PRECISION-MICRO\": [precision_micro_test], \n","                    \"RECALL-MICRO\":[recall_micro_test],\n","                    \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 22s 227ms/step - loss: 0.5166 - accuracy: 0.1336 - val_loss: 0.3133 - val_accuracy: 0.2879\n","Epoch 2/10\n","28/28 [==============================] - 6s 200ms/step - loss: 0.2899 - accuracy: 0.3605 - val_loss: 0.2485 - val_accuracy: 0.5097\n","Epoch 3/10\n","28/28 [==============================] - 6s 201ms/step - loss: 0.2252 - accuracy: 0.5595 - val_loss: 0.2043 - val_accuracy: 0.5901\n","Epoch 4/10\n","28/28 [==============================] - 6s 201ms/step - loss: 0.1763 - accuracy: 0.6406 - val_loss: 0.1701 - val_accuracy: 0.6213\n","Epoch 5/10\n","28/28 [==============================] - 6s 202ms/step - loss: 0.1405 - accuracy: 0.7105 - val_loss: 0.1505 - val_accuracy: 0.6641\n","Epoch 6/10\n","28/28 [==============================] - 8s 291ms/step - loss: 0.1146 - accuracy: 0.7651 - val_loss: 0.1401 - val_accuracy: 0.6952\n","Epoch 7/10\n","28/28 [==============================] - 6s 202ms/step - loss: 0.0938 - accuracy: 0.8017 - val_loss: 0.1351 - val_accuracy: 0.7043\n","Epoch 8/10\n","28/28 [==============================] - 6s 201ms/step - loss: 0.0793 - accuracy: 0.8071 - val_loss: 0.1349 - val_accuracy: 0.7121\n","Epoch 9/10\n","28/28 [==============================] - 6s 202ms/step - loss: 0.0665 - accuracy: 0.8363 - val_loss: 0.1360 - val_accuracy: 0.7237\n","Epoch 10/10\n","28/28 [==============================] - 6s 200ms/step - loss: 0.0533 - accuracy: 0.8452 - val_loss: 0.1370 - val_accuracy: 0.7056\n","=================================================================================\n","Accuracy train:  0.8682413204325555\n","Accuracy dev:  0.5590142671854734\n","Accuracy test:  0.5732714138286894\n","F1 macro test:  0.7532279698389761\n","Recall macro test:  0.7067903819189897\n","Precision macro test:  0.8145288315712814\n","F1 micro test:  0.7712574850299402\n","Recall micro test:  0.7348801825789274\n","Precision micro test:  0.8114237715245695\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uhnAklHDUV1p"},"source":["# Model---Text-CNN---Wikipedia Word2vec"]},{"cell_type":"code","metadata":{"id":"y6OKVLotUfo0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625020252561,"user_tz":-420,"elapsed":97552,"user":{"displayName":"Huy Lam Gia","photoUrl":"","userId":"16757804681341342086"}},"outputId":"f42f5d74-2ad2-4542-9042-fb4e1458023c"},"source":["x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","def __get_embedding_dict(model_filepath):\n","    embedding_dict = {}\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    vocab = [(word, word2vec_model.wv[word]) for word, vectors in word2vec_model.wv.vocab.items()]\n","\n","    for i in range(len(vocab)):\n","        word = vocab[i][0]\n","        vectors = vocab[i][1]\n","        embedding_dict[word] = vectors\n","\n","    return embedding_dict\n","\n","def create_embedding_matrix(model_filepath, word2id):\n","\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    embeddings_dict = __get_embedding_dict(model_filepath)\n","    embedding_matrix = np.zeros((len(word2id) + 1, word2vec_model.vector_size))\n","    for word, idx in word2id.items():\n","        embedding_vector = embeddings_dict.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[idx] = embedding_vector\n","\n","    return embedding_matrix \n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","maxlen = 30\n","embedding_dim = 400\n","vocab_size = len(tokenizer.word_index) + 1\n","model_filepath = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Word_Embedding/wiki.vi.model.bin'\n","embedding_matrix = create_embedding_matrix(model_filepath, tokenizer.word_index)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Conv1D(128, 3, activation = 'relu'))\n","model.add(GlobalMaxPool1D())\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics= ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[4]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                    \"PRECISION-MICRO\": [precision_micro_test], \n","                    \"RECALL-MICRO\":[recall_micro_test],\n","                    \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = 'a')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 9s 276ms/step - loss: 0.4926 - accuracy: 0.2979 - val_loss: 0.2279 - val_accuracy: 0.5668\n","Epoch 2/10\n","28/28 [==============================] - 7s 265ms/step - loss: 0.1945 - accuracy: 0.6354 - val_loss: 0.1874 - val_accuracy: 0.6278\n","Epoch 3/10\n","28/28 [==============================] - 7s 260ms/step - loss: 0.1488 - accuracy: 0.7060 - val_loss: 0.1712 - val_accuracy: 0.6265\n","Epoch 4/10\n","28/28 [==============================] - 7s 264ms/step - loss: 0.1216 - accuracy: 0.7526 - val_loss: 0.1628 - val_accuracy: 0.6485\n","Epoch 5/10\n","28/28 [==============================] - 8s 269ms/step - loss: 0.0989 - accuracy: 0.7854 - val_loss: 0.1601 - val_accuracy: 0.6615\n","Epoch 6/10\n","28/28 [==============================] - 7s 265ms/step - loss: 0.0832 - accuracy: 0.8100 - val_loss: 0.1612 - val_accuracy: 0.6537\n","Epoch 7/10\n","28/28 [==============================] - 7s 266ms/step - loss: 0.0679 - accuracy: 0.8201 - val_loss: 0.1656 - val_accuracy: 0.6680\n","Epoch 8/10\n","28/28 [==============================] - 7s 265ms/step - loss: 0.0563 - accuracy: 0.8440 - val_loss: 0.1692 - val_accuracy: 0.6563\n","Epoch 9/10\n","28/28 [==============================] - 7s 266ms/step - loss: 0.0459 - accuracy: 0.8433 - val_loss: 0.1714 - val_accuracy: 0.6576\n","Epoch 10/10\n","28/28 [==============================] - 7s 261ms/step - loss: 0.0361 - accuracy: 0.8645 - val_loss: 0.1781 - val_accuracy: 0.6420\n","=================================================================================\n","Accuracy train:  0.9472111553784861\n","Accuracy dev:  0.490272373540856\n","Accuracy test:  0.5098039215686274\n","F1 macro test:  0.7075818918330931\n","Recall macro test:  0.6466805865227209\n","Precision macro test:  0.785368940606768\n","F1 micro test:  0.7161263507896926\n","Recall micro test:  0.6553822746291366\n","Precision micro test:  0.7892808062299588\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bNXkEYgyUih6"},"source":["# Model---Text-CNN---BPEmb"]},{"cell_type":"code","metadata":{"id":"Z6gNeW0RUrfC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624969783668,"user_tz":-420,"elapsed":95393,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"405e5afb-7e7b-4093-cce1-666e4af23faf"},"source":["!pip install bpemb\n","\n","x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","from bpemb import BPEmb\n","bpemb_vi = BPEmb(lang = 'vi', dim = 300, vs = 200000)\n","\n","def __get_embedding_dict(model_filepath):\n","    embedding_dict = {}\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    vocab = [(word, word2vec_model.wv[word]) for word, vectors in word2vec_model.wv.vocab.items()]\n","    for i in range(len(vocab)):\n","        word = vocab[i][0]\n","        vectors = vocab[i][1]\n","        embedding_dict[word] = vectors\n","    return embedding_dict\n","\n","def create_embedding_matrix(model_filepath, word2id):\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    embeddings_dict = __get_embedding_dict(model_filepath)\n","    embedding_matrix = np.zeros((len(word2id) + 1, word2vec_model.vector_size))\n","    for word, idx in word2id.items():\n","        embedding_vector = embeddings_dict.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[idx] = embedding_vector\n","    return embedding_matrix \n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","maxlen = 30\n","embedding_dim = 300\n","vocab_size = len(tokenizer.word_index) + 1\n","model_filepath = bpemb_vi.emb_file\n","embedding_matrix = create_embedding_matrix(model_filepath, tokenizer.word_index)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Conv1D(128, 3, activation = 'relu'))\n","model.add(GlobalMaxPool1D())\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics= ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[5]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                    \"PRECISION-MICRO\": [precision_micro_test], \n","                    \"RECALL-MICRO\":[recall_micro_test],\n","                    \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting bpemb\n","  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 11.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n","Installing collected packages: sentencepiece, bpemb\n","Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n","downloading https://nlp.h-its.org/bpemb/vi/vi.wiki.bpe.vs200000.model\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3659179/3659179 [00:00<00:00, 3662717.45B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading https://nlp.h-its.org/bpemb/vi/vi.wiki.bpe.vs200000.d300.w2v.bin.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 223674818/223674818 [00:15<00:00, 14679520.72B/s]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 7s 206ms/step - loss: 0.4732 - accuracy: 0.1598 - val_loss: 0.2737 - val_accuracy: 0.4630\n","Epoch 2/10\n","28/28 [==============================] - 5s 193ms/step - loss: 0.2469 - accuracy: 0.5228 - val_loss: 0.2196 - val_accuracy: 0.5668\n","Epoch 3/10\n","28/28 [==============================] - 6s 197ms/step - loss: 0.1891 - accuracy: 0.6422 - val_loss: 0.1888 - val_accuracy: 0.6018\n","Epoch 4/10\n","28/28 [==============================] - 5s 195ms/step - loss: 0.1535 - accuracy: 0.6972 - val_loss: 0.1684 - val_accuracy: 0.6304\n","Epoch 5/10\n","28/28 [==============================] - 5s 192ms/step - loss: 0.1310 - accuracy: 0.7330 - val_loss: 0.1583 - val_accuracy: 0.6407\n","Epoch 6/10\n","28/28 [==============================] - 5s 195ms/step - loss: 0.1075 - accuracy: 0.7712 - val_loss: 0.1533 - val_accuracy: 0.6498\n","Epoch 7/10\n","28/28 [==============================] - 5s 195ms/step - loss: 0.0896 - accuracy: 0.7998 - val_loss: 0.1488 - val_accuracy: 0.6628\n","Epoch 8/10\n","28/28 [==============================] - 5s 195ms/step - loss: 0.0782 - accuracy: 0.8131 - val_loss: 0.1483 - val_accuracy: 0.6770\n","Epoch 9/10\n","28/28 [==============================] - 5s 193ms/step - loss: 0.0636 - accuracy: 0.8357 - val_loss: 0.1483 - val_accuracy: 0.6757\n","Epoch 10/10\n","28/28 [==============================] - 5s 193ms/step - loss: 0.0535 - accuracy: 0.8378 - val_loss: 0.1525 - val_accuracy: 0.6900\n","=================================================================================\n","Accuracy train:  0.8945645987478656\n","Accuracy dev:  0.5201037613488976\n","Accuracy test:  0.5165118679050568\n","F1 macro test:  0.7194828773335206\n","Recall macro test:  0.6634977952480942\n","Precision macro test:  0.7950559355273973\n","F1 micro test:  0.7393442622950819\n","Recall micro test:  0.6861924686192469\n","Precision micro test:  0.8014215904042647\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O8EeDz9oUyeg"},"source":["# Model---Text-CNN---MULTI_WC_F_E_B"]},{"cell_type":"code","metadata":{"id":"GcSZ_cNwUy9P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624969853682,"user_tz":-420,"elapsed":70051,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"485b5728-78a3-4a14-ca07-a53fc6978257"},"source":["x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","# Embedding matrix\n","def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  \n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype = np.float32)[:embedding_dim]\n","    return embedding_matrix\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","maxlen = 30\n","embedding_dim = 300\n","word_vector = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Word_Embedding/MULTI_W_F_B_E.vec'\n","embedding_matrix = create_embedding_matrix(word_vector, tokenizer.word_index, embedding_dim)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Conv1D(128, 3, activation = 'relu'))\n","model.add(GlobalMaxPool1D())\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer = 'Adam', metrics= ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[6]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 7s 206ms/step - loss: 0.5327 - accuracy: 0.0968 - val_loss: 0.3243 - val_accuracy: 0.2503\n","Epoch 2/10\n","28/28 [==============================] - 5s 192ms/step - loss: 0.3040 - accuracy: 0.3403 - val_loss: 0.2577 - val_accuracy: 0.4617\n","Epoch 3/10\n","28/28 [==============================] - 5s 193ms/step - loss: 0.2358 - accuracy: 0.5244 - val_loss: 0.2092 - val_accuracy: 0.5901\n","Epoch 4/10\n","28/28 [==============================] - 5s 192ms/step - loss: 0.1843 - accuracy: 0.6440 - val_loss: 0.1754 - val_accuracy: 0.6420\n","Epoch 5/10\n","28/28 [==============================] - 5s 193ms/step - loss: 0.1479 - accuracy: 0.7080 - val_loss: 0.1549 - val_accuracy: 0.6796\n","Epoch 6/10\n","28/28 [==============================] - 5s 191ms/step - loss: 0.1221 - accuracy: 0.7664 - val_loss: 0.1422 - val_accuracy: 0.6861\n","Epoch 7/10\n","28/28 [==============================] - 5s 191ms/step - loss: 0.0999 - accuracy: 0.7964 - val_loss: 0.1382 - val_accuracy: 0.7017\n","Epoch 8/10\n","28/28 [==============================] - 5s 190ms/step - loss: 0.0829 - accuracy: 0.8265 - val_loss: 0.1385 - val_accuracy: 0.7082\n","Epoch 9/10\n","28/28 [==============================] - 5s 189ms/step - loss: 0.0710 - accuracy: 0.8339 - val_loss: 0.1377 - val_accuracy: 0.6874\n","Epoch 10/10\n","28/28 [==============================] - 5s 192ms/step - loss: 0.0587 - accuracy: 0.8398 - val_loss: 0.1408 - val_accuracy: 0.7004\n","=================================================================================\n","Accuracy train:  0.8446215139442231\n","Accuracy dev:  0.5395590142671854\n","Accuracy test:  0.5500515995872033\n","F1 macro test:  0.737891249309725\n","Recall macro test:  0.674006760553494\n","Precision macro test:  0.8228771061077259\n","F1 micro test:  0.7589891103349086\n","Recall micro test:  0.702548497527577\n","Precision micro test:  0.8252904378909741\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KbI_9hZDk4Zl"},"source":["# ============================================="]},{"cell_type":"markdown","metadata":{"id":"6-n2O6qvt6H7"},"source":["# Model---Bi-GRU---FastText"]},{"cell_type":"code","metadata":{"id":"a1TAu5bEVG9h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624970055965,"user_tz":-420,"elapsed":202310,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"a754bf42-a7f2-47ec-9967-e6e2b9da6ad9"},"source":["x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","# Embedding matrix\n","def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  \n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype = np.float32)[:embedding_dim]\n","    return embedding_matrix\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","maxlen = 30\n","embedding_dim = 300\n","word_vector = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Word_Embedding/cc.vi.300.vec'\n","embedding_matrix = create_embedding_matrix(word_vector, tokenizer.word_index, embedding_dim)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Bidirectional(GRU(50, return_sequences=True)))\n","model.add(GlobalMaxPool1D())\n","model.add(Dropout(0.1))\n","model.add(Dense(50, activation=\"relu\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation = 'sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[7]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 18s 265ms/step - loss: 0.6000 - accuracy: 0.0860 - val_loss: 0.3550 - val_accuracy: 0.2503\n","Epoch 2/10\n","28/28 [==============================] - 6s 218ms/step - loss: 0.3377 - accuracy: 0.2581 - val_loss: 0.3066 - val_accuracy: 0.2905\n","Epoch 3/10\n","28/28 [==============================] - 6s 216ms/step - loss: 0.2926 - accuracy: 0.3229 - val_loss: 0.2500 - val_accuracy: 0.4345\n","Epoch 4/10\n","28/28 [==============================] - 6s 217ms/step - loss: 0.2361 - accuracy: 0.4725 - val_loss: 0.2110 - val_accuracy: 0.5577\n","Epoch 5/10\n","28/28 [==============================] - 6s 216ms/step - loss: 0.1925 - accuracy: 0.5825 - val_loss: 0.1818 - val_accuracy: 0.6083\n","Epoch 6/10\n","28/28 [==============================] - 6s 217ms/step - loss: 0.1608 - accuracy: 0.6544 - val_loss: 0.1652 - val_accuracy: 0.6472\n","Epoch 7/10\n","28/28 [==============================] - 6s 217ms/step - loss: 0.1409 - accuracy: 0.7156 - val_loss: 0.1541 - val_accuracy: 0.6822\n","Epoch 8/10\n","28/28 [==============================] - 6s 216ms/step - loss: 0.1236 - accuracy: 0.7606 - val_loss: 0.1453 - val_accuracy: 0.6887\n","Epoch 9/10\n","28/28 [==============================] - 6s 217ms/step - loss: 0.1083 - accuracy: 0.7792 - val_loss: 0.1434 - val_accuracy: 0.7056\n","Epoch 10/10\n","28/28 [==============================] - 6s 218ms/step - loss: 0.0962 - accuracy: 0.7952 - val_loss: 0.1406 - val_accuracy: 0.7302\n","=================================================================================\n","Accuracy train:  0.7474388161639157\n","Accuracy dev:  0.5577172503242542\n","Accuracy test:  0.5789473684210527\n","F1 macro test:  0.749154476471316\n","Recall macro test:  0.7167794643177632\n","Precision macro test:  0.8002776994130087\n","F1 micro test:  0.7670037675986516\n","Recall micro test:  0.7356409281095474\n","Precision micro test:  0.8011599005799502\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WWI33_wiKB9R"},"source":["# Model---Bi-GRU---Wikipedia Word2vec"]},{"cell_type":"code","metadata":{"id":"Uy2kJ6igKCO6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624970156840,"user_tz":-420,"elapsed":100895,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"862f7d40-4043-41d8-d756-ff13ecb720ab"},"source":["x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","def __get_embedding_dict(model_filepath):\n","    embedding_dict = {}\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    vocab = [(word, word2vec_model.wv[word]) for word, vectors in word2vec_model.wv.vocab.items()]\n","\n","    for i in range(len(vocab)):\n","        word = vocab[i][0]\n","        vectors = vocab[i][1]\n","        embedding_dict[word] = vectors\n","\n","    return embedding_dict\n","\n","def create_embedding_matrix(model_filepath, word2id):\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    embeddings_dict = __get_embedding_dict(model_filepath)\n","    embedding_matrix = np.zeros((len(word2id) + 1, word2vec_model.vector_size))\n","    for word, idx in word2id.items():\n","        embedding_vector = embeddings_dict.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[idx] = embedding_vector\n","\n","    return embedding_matrix \n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","maxlen = 30\n","embedding_dim = 400\n","vocab_size = len(tokenizer.word_index) + 1\n","model_filepath = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Word_Embedding/wiki.vi.model.bin'\n","embedding_matrix = create_embedding_matrix(model_filepath, tokenizer.word_index)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Bidirectional(GRU(50, return_sequences=True)))\n","model.add(GlobalMaxPool1D())\n","model.add(Dropout(0.1))\n","model.add(Dense(50, activation=\"relu\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation = 'sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[8]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 14s 316ms/step - loss: 0.5009 - accuracy: 0.2177 - val_loss: 0.3059 - val_accuracy: 0.3372\n","Epoch 2/10\n","28/28 [==============================] - 7s 265ms/step - loss: 0.2940 - accuracy: 0.3427 - val_loss: 0.2526 - val_accuracy: 0.4643\n","Epoch 3/10\n","28/28 [==============================] - 8s 269ms/step - loss: 0.2394 - accuracy: 0.4791 - val_loss: 0.2014 - val_accuracy: 0.5681\n","Epoch 4/10\n","28/28 [==============================] - 7s 267ms/step - loss: 0.1919 - accuracy: 0.6004 - val_loss: 0.1739 - val_accuracy: 0.6148\n","Epoch 5/10\n","28/28 [==============================] - 8s 270ms/step - loss: 0.1641 - accuracy: 0.6527 - val_loss: 0.1607 - val_accuracy: 0.6472\n","Epoch 6/10\n","28/28 [==============================] - 8s 268ms/step - loss: 0.1395 - accuracy: 0.7134 - val_loss: 0.1525 - val_accuracy: 0.6654\n","Epoch 7/10\n","28/28 [==============================] - 8s 269ms/step - loss: 0.1272 - accuracy: 0.7402 - val_loss: 0.1460 - val_accuracy: 0.6861\n","Epoch 8/10\n","28/28 [==============================] - 7s 266ms/step - loss: 0.1147 - accuracy: 0.7712 - val_loss: 0.1420 - val_accuracy: 0.6861\n","Epoch 9/10\n","28/28 [==============================] - 7s 266ms/step - loss: 0.1038 - accuracy: 0.7849 - val_loss: 0.1436 - val_accuracy: 0.6926\n","Epoch 10/10\n","28/28 [==============================] - 7s 264ms/step - loss: 0.0968 - accuracy: 0.7908 - val_loss: 0.1406 - val_accuracy: 0.6732\n","=================================================================================\n","Accuracy train:  0.749003984063745\n","Accuracy dev:  0.5512321660181583\n","Accuracy test:  0.5701754385964912\n","F1 macro test:  0.7409694251276097\n","Recall macro test:  0.696362466849158\n","Precision macro test:  0.8017704479955808\n","F1 micro test:  0.7612748045700543\n","Recall micro test:  0.7223278813236972\n","Precision micro test:  0.8046610169491526\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DU5w1OPUVU9u"},"source":["# Model---Bi-GRU---BPEmb"]},{"cell_type":"code","metadata":{"id":"bgKa1QlWOVDB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624970241166,"user_tz":-420,"elapsed":84340,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"f8ec4a51-8339-419e-df4e-8e7acef13f7c"},"source":["!pip install bpemb\n","\n","x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","from bpemb import BPEmb\n","bpemb_vi = BPEmb(lang = 'vi', dim = 300, vs = 200000)\n","\n","def __get_embedding_dict(model_filepath):\n","    embedding_dict = {}\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    vocab = [(word, word2vec_model.wv[word]) for word, vectors in word2vec_model.wv.vocab.items()]\n","\n","    for i in range(len(vocab)):\n","        word = vocab[i][0]\n","        vectors = vocab[i][1]\n","        embedding_dict[word] = vectors\n","\n","    return embedding_dict\n","\n","def create_embedding_matrix(model_filepath, word2id):\n","    \"\"\"\n","    Get the embedding matrix of the word2vec model\n","    :param model_filepath: the file path to the pre-build word2vec model\n","    :param word2id: the directory mapping from word to id\n","    :return: the embedding matrix of the word2vec model\n","    \"\"\"\n","    word2vec_model = KeyedVectors.load_word2vec_format(model_filepath, binary=True)\n","    embeddings_dict = __get_embedding_dict(model_filepath)\n","    embedding_matrix = np.zeros((len(word2id) + 1, word2vec_model.vector_size))\n","    for word, idx in word2id.items():\n","        embedding_vector = embeddings_dict.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[idx] = embedding_vector\n","\n","    return embedding_matrix \n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","maxlen = 30\n","embedding_dim = 300\n","vocab_size = len(tokenizer.word_index) + 1\n","model_filepath = bpemb_vi.emb_file\n","embedding_matrix = create_embedding_matrix(model_filepath, tokenizer.word_index)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Bidirectional(GRU(50, return_sequences=True)))\n","model.add(GlobalMaxPool1D())\n","model.add(Dropout(0.1))\n","model.add(Dense(50, activation=\"relu\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation = 'sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[9]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bpemb in /usr/local/lib/python3.7/dist-packages (0.3.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from bpemb) (0.1.96)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.1.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.5.30)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  del sys.path[0]\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 12s 259ms/step - loss: 0.5816 - accuracy: 0.1211 - val_loss: 0.3387 - val_accuracy: 0.2555\n","Epoch 2/10\n","28/28 [==============================] - 6s 215ms/step - loss: 0.3238 - accuracy: 0.2828 - val_loss: 0.2784 - val_accuracy: 0.4228\n","Epoch 3/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.2627 - accuracy: 0.4406 - val_loss: 0.2201 - val_accuracy: 0.5409\n","Epoch 4/10\n","28/28 [==============================] - 6s 214ms/step - loss: 0.2101 - accuracy: 0.5592 - val_loss: 0.1872 - val_accuracy: 0.6018\n","Epoch 5/10\n","28/28 [==============================] - 6s 217ms/step - loss: 0.1780 - accuracy: 0.6169 - val_loss: 0.1663 - val_accuracy: 0.6368\n","Epoch 6/10\n","28/28 [==============================] - 6s 217ms/step - loss: 0.1500 - accuracy: 0.6863 - val_loss: 0.1499 - val_accuracy: 0.6602\n","Epoch 7/10\n","28/28 [==============================] - 6s 215ms/step - loss: 0.1302 - accuracy: 0.7281 - val_loss: 0.1429 - val_accuracy: 0.6822\n","Epoch 8/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.1154 - accuracy: 0.7536 - val_loss: 0.1388 - val_accuracy: 0.7004\n","Epoch 9/10\n","28/28 [==============================] - 6s 215ms/step - loss: 0.1045 - accuracy: 0.7789 - val_loss: 0.1393 - val_accuracy: 0.7069\n","Epoch 10/10\n","28/28 [==============================] - 6s 215ms/step - loss: 0.0944 - accuracy: 0.8119 - val_loss: 0.1363 - val_accuracy: 0.7069\n","=================================================================================\n","Accuracy train:  0.7524188958451906\n","Accuracy dev:  0.5525291828793775\n","Accuracy test:  0.5815273477812177\n","F1 macro test:  0.7601800206119603\n","Recall macro test:  0.7200633616456956\n","Precision macro test:  0.813332857530352\n","F1 micro test:  0.7735358784729163\n","Recall micro test:  0.7360213008748574\n","Precision micro test:  0.8150800336983993\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wpXpcrP0VeBl"},"source":["# Model---Bi-GRU---MULTI_WC_F_E_B"]},{"cell_type":"code","metadata":{"id":"q0DSWWIrVjpS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624970329186,"user_tz":-420,"elapsed":88061,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"f4f4baa3-2123-412f-e0c1-834f5937933a"},"source":["x_train = X_train\n","x_dev = X_dev\n","x_test = X_test\n","\n","# Embedding matrix\n","def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  \n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype = np.float32)[:embedding_dim]\n","    return embedding_matrix\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(x_train)\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","maxlen = 30\n","embedding_dim = 300\n","word_vector = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Word_Embedding/MULTI_W_F_B_E.vec'\n","embedding_matrix = create_embedding_matrix(word_vector, tokenizer.word_index, embedding_dim)\n","\n","x_train = tokenizer.texts_to_sequences(x_train)\n","x_dev = tokenizer.texts_to_sequences(x_dev)\n","x_test = tokenizer.texts_to_sequences(x_test)\n","\n","x_train = pad_sequences(x_train, maxlen = maxlen, truncating='post', padding='post')\n","x_dev = pad_sequences(x_dev, maxlen = maxlen, truncating='post', padding='post')\n","x_test = pad_sequences(x_test, maxlen = maxlen, truncating='post', padding='post')\n","\n","n_inputs, n_outputs = x_train.shape[1], y_train.shape[1]\n","\n","epochs = 10\n","batch_size = 256\n","\n","model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, \n","                    weights = [embedding_matrix], input_length = maxlen, trainable = True))\n","model.add(Bidirectional(GRU(50, return_sequences=True)))\n","model.add(GlobalMaxPool1D())\n","model.add(Dropout(0.1))\n","model.add(Dense(50, activation=\"relu\"))\n","model.add(Dropout(0.1))\n","model.add(Dense(128, activation = 'relu'))\n","model.add(Dense(n_outputs, activation = 'sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","# model.summary()\n","\n","model.fit(x_train, y_train, verbose=1, epochs=epochs, batch_size = batch_size, validation_data = (x_dev, y_dev))\n","\n","y_train_pred = model.predict(x_train).round() \n","y_dev_pred = model.predict(x_dev).round() \n","y_test_pred = model.predict(x_test).round() \n","\n","acc_train = accuracy_score(y_train, y_train_pred)\n","acc_dev = accuracy_score(y_dev, y_dev_pred)\n","acc_test = accuracy_score(y_test, y_test_pred)\n","f1_macro_test = f1_score(y_test, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[10]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","28/28 [==============================] - 12s 257ms/step - loss: 0.6280 - accuracy: 0.0852 - val_loss: 0.3711 - val_accuracy: 0.2192\n","Epoch 2/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.3508 - accuracy: 0.2361 - val_loss: 0.3254 - val_accuracy: 0.2503\n","Epoch 3/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.3179 - accuracy: 0.2456 - val_loss: 0.2995 - val_accuracy: 0.3074\n","Epoch 4/10\n","28/28 [==============================] - 6s 212ms/step - loss: 0.2834 - accuracy: 0.3375 - val_loss: 0.2463 - val_accuracy: 0.3956\n","Epoch 5/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.2311 - accuracy: 0.4559 - val_loss: 0.2031 - val_accuracy: 0.5422\n","Epoch 6/10\n","28/28 [==============================] - 6s 212ms/step - loss: 0.1840 - accuracy: 0.5992 - val_loss: 0.1763 - val_accuracy: 0.5694\n","Epoch 7/10\n","28/28 [==============================] - 6s 214ms/step - loss: 0.1537 - accuracy: 0.6516 - val_loss: 0.1633 - val_accuracy: 0.6200\n","Epoch 8/10\n","28/28 [==============================] - 6s 212ms/step - loss: 0.1277 - accuracy: 0.7191 - val_loss: 0.1593 - val_accuracy: 0.6498\n","Epoch 9/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.1147 - accuracy: 0.7527 - val_loss: 0.1571 - val_accuracy: 0.6719\n","Epoch 10/10\n","28/28 [==============================] - 6s 213ms/step - loss: 0.0993 - accuracy: 0.7888 - val_loss: 0.1536 - val_accuracy: 0.7004\n","=================================================================================\n","Accuracy train:  0.7383323847467274\n","Accuracy dev:  0.5343709468223087\n","Accuracy test:  0.5531475748194015\n","F1 macro test:  0.729298182698826\n","Recall macro test:  0.687589673771536\n","Precision macro test:  0.7929520003741964\n","F1 micro test:  0.7555378168030333\n","Recall micro test:  0.7200456447318372\n","Precision micro test:  0.7947103274559194\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XQCXgytVp-OX"},"source":["# ========================================================="]},{"cell_type":"code","metadata":{"id":"59j3yHlF80X9"},"source":["max_len = max([len(text.split(\" \")) for text in X_train])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqHSmChDNRy6"},"source":["# BERT XLM-R"]},{"cell_type":"code","metadata":{"id":"KryMKxjgNSQq","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["08a4988fbe744812b0aec29a56927f68","54b31fa1db4d4a94b2d485d590582ff9","f7df7edff9d9421e9b188d2d3f49843b","b9ce59b412ba44db9a62443ae149be3b","8937ae0b527e4d1183e34cc5a2618a5d","0ee06b0511d543338fcd4926b993af93","6b8096596bb6474aaee9b9b2f902a6d0","2af44925097d44afb022e12baed89311","fc69b7e308de4519b39f388f5d7349d0","0b882f24a5bf476c924fb8129731f98f","74a686ca4baf448684595f39230c17ac","e4ef84429a3a4a5894f9a2b5ba225811","ce60b41d64f94ac692fe361dda865509","c51801d501844e90b5c1f27d0af2030d","837ace3696ce48cdba8fb9d51bfd57fc","7a00005563f94b6db064c2dea396fd99","91726d00b78a425cad7197ff7443f8c3","fd19ff87455f4c618cc0c0eb3e69b448","285e83eed3dd49cc9674fcb98a51454d","0a8fd5a1eff74e8bac36be8930d3a3b6","11444ea1a3cc4bf4bf7245eafcb0ca5b","ee5d5d06e80040088640f3a440089ee3","3e45ffce5ebb49ac959a74ce0ea08cf5","c9b9d6eaf2f04230b37d9cfa38b0571b","5f65a1be7671423f824058d7daf9e019","2d78c9f01b1946049992c54e9406fa19","3952cc1699824d1da52ed5d68f9acec0","80963d039d50426faad3d428e4ebe595","5ebcb8675d7b4e12a7da73309c3b08f8","65ea25a08d1e49aa86236726f158ab89","0b94c56c63294d778f25c52a97bd71e2","945af86fb6e0470e9da2dd3c712b15f3"]},"executionInfo":{"status":"ok","timestamp":1624974217688,"user_tz":-420,"elapsed":997752,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"eaa4a641-ae7b-422e-d0b7-fe1b6ead2745"},"source":["max_len = max([len(text.split(\" \")) for text in X_train])\n","class BuildDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, tokenizer, X, y, max_len):\n","        self.tokenizer = tokenizer\n","        self.comment_text = X\n","        self.targets = y\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, index):\n","        comment_text = str(self.comment_text[index])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'labels': torch.tensor(self.targets[index], dtype=torch.float)\n","        }\n","\n","# Khai bao pre-trained\n","model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels = 12)\n","tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n","\n","# Chuan bi data\n","train_dataset = BuildDataset(tokenizer, X_train, y_train, max_len = max_len)\n","dev_dataset = BuildDataset(tokenizer,  X_dev, y_dev, max_len = max_len)\n","test_dataset = BuildDataset(tokenizer,  X_test, y_test, max_len = max_len)\n","\n","# Chuan bi mo hinh\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs = 7,              \n","    per_device_train_batch_size=16,  \n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,                \n","    weight_decay=0.01,\n","    learning_rate = 8e-05,\n","    no_cuda=False\n",")\n","\n","trainer = Trainer(\n","    model=model,                         \n","    args=training_args,                  \n","    train_dataset=train_dataset,         \n","    eval_dataset=dev_dataset             \n",")\n","\n","trainer.train()\n","\n","train_dataset_pred = trainer.predict(train_dataset)\n","dev_dataset_pred = trainer.predict(dev_dataset)\n","test_dataset_pred = trainer.predict(test_dataset)\n","\n","def convert_predict_label(predict_label_train):\n","    for i in range(len(predict_label_train)):\n","        for j in range(len(predict_label_train[i])):\n","            if predict_label_train[i][j] > 0.5:\n","                predict_label_train[i][j] = 1\n","            else:\n","                predict_label_train[i][j] = 0\n","    return predict_label_train\n","\n","y_train_pred = convert_predict_label(train_dataset_pred.predictions)\n","y_dev_pred = convert_predict_label(dev_dataset_pred.predictions)\n","y_test_pred = convert_predict_label(test_dataset_pred.predictions)\n","\n","y_train_true = train_dataset_pred.label_ids\n","y_dev_true = dev_dataset_pred.label_ids\n","y_test_true = test_dataset_pred.label_ids\n","\n","# Danh gia mo hinh\n","acc_train = accuracy_score(y_train_true, y_train_pred)\n","acc_dev = accuracy_score(y_dev_true, y_dev_pred)\n","acc_test = accuracy_score(y_test_true, y_test_pred)\n","f1_macro_test = f1_score(y_test_true, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test_true, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test_true, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test_true, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test_true, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test_true, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu dự đoán\n","result = pd.DataFrame([X_test, y_test_true, y_test_pred]).T\n","result.columns = ['text', 'true_label', 'pred_label']\n","path = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Predict CSV/ACD_Process'\n","result.to_csv(path + '/' + 'xlm-r.csv', index=False)\n","result.head()\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[11]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"08a4988fbe744812b0aec29a56927f68","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc69b7e308de4519b39f388f5d7349d0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91726d00b78a425cad7197ff7443f8c3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f65a1be7671423f824058d7daf9e019","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9096718.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 7028\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3080\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3080' max='3080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3080/3080 15:26, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.307700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.174900</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.136900</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.112600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.089400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.071300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 7028\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='611' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [440/440 00:28]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 771\n","  Batch size = 16\n","***** Running Prediction *****\n","  Num examples = 1938\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.8373648264086511\n","Accuracy dev:  0.5979247730220493\n","Accuracy test:  0.6444788441692466\n","F1 macro test:  0.7819406583694389\n","Recall macro test:  0.7493981651529175\n","Precision macro test:  0.822271014330635\n","F1 micro test:  0.7949690556997405\n","Recall micro test:  0.7573221757322176\n","Precision micro test:  0.8365546218487395\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zTj8-w0r9KIY"},"source":["# BERT MULTILINGUAL"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5cff0b7173ab4767856b9d18f967621e","e9a8e9243a2e42bf94bcd81f49d1d1be","da6de0458b8b46fd97db4779f4348d92","186ac397bd764e92beeb6282f496ce74","a1acd417f2ff41f78a68d5f243f4e25c","bb62c5551b1143b281e544cfaab43323","46b4fb6d603a4181a0b9c73fa4cc8163","ef617c284b0a4ff2b033dd51878a27e5","ccc38a6eced84613b20d5f1854708818","27f3fbb23b494f71802782abe0c6b1b2","4c24dd06cea34ff28c25d4de2a5c9755","7c04665e425945739c824ecf60d2f672","5aa70cc105b54bc5b19ae8cd916a3c9f","104c116b06fa4a7487f8e1afd297cf83","7e13276f183a40a4910cfa2873f3e73d","15e08a0c22c943eaabde6ec1fa1f6ea0","73fc091bc6f84b07a75fac05b5d7c79a","91edbef317b948e0ab03ac42a5ba1971","ff9f2c1e18574a9ba73c346010e8c2a5","6f15262557bf4c00b3167d40a334b4e3","1c1856b96d1b4671bd929cde3da59e7c","268778f492df43a4a144fbdd1b5a2d4b","1475f8b3d13541e493c148c030b91caf","0e7f690b1c384e42b0aaa476f3d8eedd","cacf895ff5f54cc38427cce05a4594dd","6199a348c51249d5802ffc9efc6e5394","dbd04a9f11994ed09dc83f3a8f77f910","fc73f3ac69b54f1e9bb00598a8025555","d2b324d6651242d0a6346ebe86c7c4ef","a5aa33b7b34c42a784f3f84172e952c9","7cb582465ed64fb6aaf5d82dbf20aa37","1707f69fcdab4883a154ff9835a414a2","decb1c4b29bc44469b62077032299965","e86db00a69384d578ea1e0dd9ebf9d13","184c58cb84534fa79ec0d91fc91c9056","0926716c2edf4ab58da026bc226b906b","9082ca1fdc1645d891b317ee4978aeeb","3b3bde3356fe44e4afa3b088b35560de","8bade341d1d64088a95487f2346efa1a","4b05e976c1fe4e51b30092da3db5afe7"]},"id":"BpCDmzEu9KI0","executionInfo":{"status":"ok","timestamp":1625058777539,"user_tz":-420,"elapsed":1295022,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"ad6846f4-4311-4d72-fa9f-5e4b48cb5f7c"},"source":["max_len = max([len(text.split(\" \")) for text in X_train])\n","class BuildDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, tokenizer, X, y, max_len):\n","        self.tokenizer = tokenizer\n","        self.comment_text = X\n","        self.targets = y\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, index):\n","        comment_text = str(self.comment_text[index])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'labels': torch.tensor(self.targets[index], dtype=torch.float)\n","        }\n","\n","# Khai bao pre-trained\n","model = AutoModelForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels = 12)\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# Chuan bi data\n","train_dataset = BuildDataset(tokenizer, X_train, y_train, max_len = max_len)\n","dev_dataset = BuildDataset(tokenizer,  X_dev, y_dev, max_len = max_len)\n","test_dataset = BuildDataset(tokenizer,  X_test, y_test, max_len = max_len)\n","\n","# Chuan bi mo hinh\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs = 7,              \n","    per_device_train_batch_size=16,  \n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,                \n","    weight_decay=0.01,\n","    learning_rate = 8e-05,\n","    no_cuda=False\n",")\n","\n","trainer = Trainer(\n","    model=model,                         \n","    args=training_args,                  \n","    train_dataset=train_dataset,         \n","    eval_dataset=dev_dataset             \n",")\n","\n","trainer.train()\n","\n","train_dataset_pred = trainer.predict(train_dataset)\n","dev_dataset_pred = trainer.predict(dev_dataset)\n","test_dataset_pred = trainer.predict(test_dataset)\n","\n","def convert_predict_label(predict_label_train):\n","    for i in range(len(predict_label_train)):\n","        for j in range(len(predict_label_train[i])):\n","            if predict_label_train[i][j] > 0.5:\n","                predict_label_train[i][j] = 1\n","            else:\n","                predict_label_train[i][j] = 0\n","    return predict_label_train\n","\n","y_train_pred = convert_predict_label(train_dataset_pred.predictions)\n","y_dev_pred = convert_predict_label(dev_dataset_pred.predictions)\n","y_test_pred = convert_predict_label(test_dataset_pred.predictions)\n","\n","y_train_true = train_dataset_pred.label_ids\n","y_dev_true = dev_dataset_pred.label_ids\n","y_test_true = test_dataset_pred.label_ids\n","\n","# Danh gia mo hinh\n","acc_train = accuracy_score(y_train_true, y_train_pred)\n","acc_dev = accuracy_score(y_dev_true, y_dev_pred)\n","acc_test = accuracy_score(y_test_true, y_test_pred)\n","f1_macro_test = f1_score(y_test_true, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test_true, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test_true, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test_true, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test_true, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test_true, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu dự đoán\n","result = pd.DataFrame([X_test, y_test_true, y_test_pred]).T\n","result.columns = ['text', 'true_label', 'pred_label']\n","path = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Predict CSV/ACD_Process'\n","result.to_csv(path + '/' + 'bert-multi.csv', index=False)\n","result.head()\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[12]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cff0b7173ab4767856b9d18f967621e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccc38a6eced84613b20d5f1854708818","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73fc091bc6f84b07a75fac05b5d7c79a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cacf895ff5f54cc38427cce05a4594dd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"decb1c4b29bc44469b62077032299965","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1961828.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 7028\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3080\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3080' max='3080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3080/3080 19:43, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.289800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.165500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.122500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.093200</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.066500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.046100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 7028\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='611' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [440/440 00:59]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 771\n","  Batch size = 16\n","***** Running Prediction *****\n","  Num examples = 1938\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.9231644849174729\n","Accuracy dev:  0.6381322957198443\n","Accuracy test:  0.6377708978328174\n","F1 macro test:  0.776327285143917\n","Recall macro test:  0.7525022080803826\n","Precision macro test:  0.8034979986881198\n","F1 micro test:  0.7906428150186751\n","Recall micro test:  0.7649296310384176\n","Precision micro test:  0.8181448331977217\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f3hAwsvUB60W"},"source":["# BERT FPTAI/vibert-base-cased\n","\n"]},{"cell_type":"code","metadata":{"id":"4-4o5FZCqbqx","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["18398673eed042d5b272405cd79a0fd9","79b3283d6ea4419aa612714c878d3836","2bb850be8e934f848c5864240bb410fb","dd185c07aa4c414cabffb4cc827bcc62","d58da5a815ae4d7db92c23cb3baff1f8","ab2b7684d4ea4d7eb17b724d862009b4","0484e91741914360a26dd20669d268a7","5a8ed1d12d1b4dfdb7e6f0617e6c7845","862b404ff3714df79b0cf2f6476289d7","c6c8a8439a0a40c08c6fa1bc979ac650","876af751a4bb44bb92c3ef5acb8e9f6e","f0133707a3ec45cc8d7a1347234a9337","9cc7d3e6aade4e498a2ce546e2bcaaf5","1709ada5746640f7a5305dcc9892183b","38d4224ed2404ee490e7013c09520c3c","2c077c82df644d0694bc6e503c655246","f44ff5a254ab49349bfb3333ea820ef0","8178bb41753e45d290dacb2c39e13e38","7b925bb64fa14f46ac86a8d8bacf6291","eda2051f8b094a838daba7d87623b433","fff17351f209481998f8b2dc1db78d7f","1d982a80e86447a2a3fd68e161fcc400","f15504c3fae546dfb5459a59dc7a9224","9026066054894123afb1308a69d12300"]},"executionInfo":{"status":"ok","timestamp":1624975270858,"user_tz":-420,"elapsed":605139,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"1239373c-3bee-40c1-9081-df9e2ecf19ec"},"source":["max_len = max([len(text.split(\" \")) for text in X_train])\n","class BuildDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, tokenizer, X, y, max_len):\n","        self.tokenizer = tokenizer\n","        self.comment_text = X\n","        self.targets = y\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, index):\n","        comment_text = str(self.comment_text[index])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'labels': torch.tensor(self.targets[index], dtype=torch.float)\n","        }\n","\n","# Khai bao pre-trained\n","model = AutoModelForSequenceClassification.from_pretrained('FPTAI/vibert-base-cased', num_labels = 12)\n","tokenizer = AutoTokenizer.from_pretrained('FPTAI/vibert-base-cased')\n","\n","# Chuan bi data\n","train_dataset = BuildDataset(tokenizer, X_train, y_train, max_len = max_len)\n","dev_dataset = BuildDataset(tokenizer,  X_dev, y_dev, max_len = max_len)\n","test_dataset = BuildDataset(tokenizer,  X_test, y_test, max_len = max_len)\n","\n","# Chuan bi mo hinh\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs = 7,              \n","    per_device_train_batch_size=16,  \n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,                \n","    weight_decay=0.01,\n","    no_cuda=False\n",")\n","\n","trainer = Trainer(\n","    model=model,                         \n","    args=training_args,                  \n","    train_dataset=train_dataset,         \n","    eval_dataset=dev_dataset             \n",")\n","\n","trainer.train()\n","\n","train_dataset_pred = trainer.predict(train_dataset)\n","dev_dataset_pred = trainer.predict(dev_dataset)\n","test_dataset_pred = trainer.predict(test_dataset)\n","\n","def convert_predict_label(predict_label_train):\n","    for i in range(len(predict_label_train)):\n","        for j in range(len(predict_label_train[i])):\n","            if predict_label_train[i][j] > 0.5:\n","                predict_label_train[i][j] = 1\n","            else:\n","                predict_label_train[i][j] = 0\n","    return predict_label_train\n","\n","y_train_pred = convert_predict_label(train_dataset_pred.predictions)\n","y_dev_pred = convert_predict_label(dev_dataset_pred.predictions)\n","y_test_pred = convert_predict_label(test_dataset_pred.predictions)\n","\n","y_train_true = train_dataset_pred.label_ids\n","y_dev_true = dev_dataset_pred.label_ids\n","y_test_true = test_dataset_pred.label_ids\n","\n","# Danh gia mo hinh\n","acc_train = accuracy_score(y_train_true, y_train_pred)\n","acc_dev = accuracy_score(y_dev_true, y_dev_pred)\n","acc_test = accuracy_score(y_test_true, y_test_pred)\n","f1_macro_test = f1_score(y_test_true, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test_true, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test_true, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test_true, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test_true, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test_true, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu dự đoán\n","result = pd.DataFrame([X_test, y_test_true, y_test_pred]).T\n","result.columns = ['text', 'true_label', 'pred_label']\n","path = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Predict CSV/ACD_Process'\n","result.to_csv(path + '/' + 'vibert-base-cased.csv', index=False)\n","result.head()\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[13]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18398673eed042d5b272405cd79a0fd9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1403.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"862b404ff3714df79b0cf2f6476289d7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=581243664.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at FPTAI/vibert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at FPTAI/vibert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f44ff5a254ab49349bfb3333ea820ef0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254703.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 7028\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3080\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3080' max='3080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3080/3080 09:02, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.353400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.180500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.124500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.085500</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.055600</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.039200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 7028\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='611' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [440/440 00:29]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 771\n","  Batch size = 16\n","***** Running Prediction *****\n","  Num examples = 1938\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.9435116676152533\n","Accuracy dev:  0.6018158236057068\n","Accuracy test:  0.6248710010319918\n","F1 macro test:  0.7657405542812664\n","Recall macro test:  0.7262557124589287\n","Precision macro test:  0.814073635640693\n","F1 micro test:  0.7801956478338989\n","Recall micro test:  0.7432483834157474\n","Precision micro test:  0.8210084033613445\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2gVKfe-hCF7z"},"source":["# BERT vinai/phobert-base"]},{"cell_type":"code","metadata":{"id":"14VIzJ65pcaq","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e71f6a2651ac4b8f814e613d899e398c","9123738699634839a6b86da7efa9c838","4b128a874a31440d93ae527446873b5f","478681fcec4846c1bbc83a2c605946d3","3df6926711134a9c814e576a69db3538","5a6bc4911c2246009638754ffa5b4dd4","988e0836b6144073a6b70e0086956024","fd1f5ce37bac4347829681cdacbb57a3","5b1a815a76b2442f98dcae52a0b90778","4b04c6e8e2724af98bed2859828645d2","8c467f778d57410ba9d4b59ad3d72caf","116ff0e774eb48a29b8409d034ee59eb","17f4dadc198a47119e534390db7ff0e5","a88f4f2a44764010ab363e45e1b31150","cbab3b7b2ffc4bd6b6ac329f32dc9a1b","6da4953feaad4e55afda481d1e6c3067","893c1d3ac1bd4056ab575cf2bb9fe4f8","f4fa37c4d7aa4d658e5460842e9e845f","9f35012bca2f4b028690836045fe19ec","834a71cb8bd14b51b9e50a62ba5b42c8","0329581fdffe436d800c367c4faae514","97e8a482f35543a885c567b60c6f43b8","8a70b664047540feaa80f6342161e0b2","460c487a6e2b42c581622255a54e62fb","5495960939ef42c7ae914b713a9e7879","0fe12bce58494e8e985acd9c28bda824","dcfb3ae2ff294fd1b65977670f9c279f","46cc127894a44ae0ae7f7ff975ed7119","9611d20ab242454e9956f431f7bcd78c","1731f43f3f5545aaaae4844b7f24fd3a","4ca5209b711d40c590003747e36314cd","f9a23f34dbaa4c949ea2a4a01d021b6b"]},"executionInfo":{"status":"ok","timestamp":1624976122287,"user_tz":-420,"elapsed":633798,"user":{"displayName":"Gia Huy Lâm","photoUrl":"","userId":"11490085767137168489"}},"outputId":"6c74089f-f961-41f4-f58e-e8145dbe7f6f"},"source":["class BuildDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, tokenizer, X, y, max_len):\n","        self.tokenizer = tokenizer\n","        self.comment_text = X\n","        self.targets = y\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.comment_text)\n","\n","    def __getitem__(self, index):\n","        comment_text = str(self.comment_text[index])\n","        comment_text = \" \".join(comment_text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            comment_text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'input_ids': torch.tensor(ids, dtype=torch.long),\n","            'attention_mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'labels': torch.tensor(self.targets[index], dtype=torch.float)\n","        }\n","\n","# Khai bao pre-trained\n","model = AutoModelForSequenceClassification.from_pretrained('vinai/phobert-base', num_labels = 12)\n","tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base')\n","\n","# Chuan bi data\n","train_dataset = BuildDataset(tokenizer, X_train, y_train, max_len = max_len)\n","dev_dataset = BuildDataset(tokenizer,  X_dev, y_dev, max_len = max_len)\n","test_dataset = BuildDataset(tokenizer,  X_test, y_test, max_len = max_len)\n","\n","# Chuan bi mo hinh\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          \n","    num_train_epochs = 7,              \n","    per_device_train_batch_size=16,  \n","    per_device_eval_batch_size=16,\n","    warmup_steps=500,                \n","    weight_decay=0.01,\n","    no_cuda=False\n",")\n","\n","trainer = Trainer(\n","    model=model,                         \n","    args=training_args,                  \n","    train_dataset=train_dataset,         \n","    eval_dataset=dev_dataset             \n",")\n","\n","trainer.train()\n","\n","train_dataset_pred = trainer.predict(train_dataset)\n","dev_dataset_pred = trainer.predict(dev_dataset)\n","test_dataset_pred = trainer.predict(test_dataset)\n","\n","def convert_predict_label(predict_label_train):\n","    for i in range(len(predict_label_train)):\n","        for j in range(len(predict_label_train[i])):\n","            if predict_label_train[i][j] > 0.5:\n","                predict_label_train[i][j] = 1\n","            else:\n","                predict_label_train[i][j] = 0\n","    return predict_label_train\n","\n","y_train_pred = convert_predict_label(train_dataset_pred.predictions)\n","y_dev_pred = convert_predict_label(dev_dataset_pred.predictions)\n","y_test_pred = convert_predict_label(test_dataset_pred.predictions)\n","\n","y_train_true = train_dataset_pred.label_ids\n","y_dev_true = dev_dataset_pred.label_ids\n","y_test_true = test_dataset_pred.label_ids\n","\n","# Danh gia mo hinh\n","acc_train = accuracy_score(y_train_true, y_train_pred)\n","acc_dev = accuracy_score(y_dev_true, y_dev_pred)\n","acc_test = accuracy_score(y_test_true, y_test_pred)\n","f1_macro_test = f1_score(y_test_true, y_test_pred, average='macro')\n","recall_macro_test = recall_score(y_test_true, y_test_pred, average='macro')\n","precision_macro_test = precision_score(y_test_true, y_test_pred, average='macro')\n","f1_micro_test = f1_score(y_test_true, y_test_pred, average='micro')\n","recall_micro_test = recall_score(y_test_true, y_test_pred, average='micro')\n","precision_micro_test = precision_score(y_test_true, y_test_pred, average='micro')\n","print('=================================================================================')\n","print('Accuracy train: ', acc_train)\n","print('Accuracy dev: ', acc_dev)\n","print('Accuracy test: ', acc_test)\n","print('F1 macro test: ', f1_macro_test)\n","print('Recall macro test: ', recall_macro_test)\n","print('Precision macro test: ', precision_macro_test)\n","print('F1 micro test: ', f1_micro_test)\n","print('Recall micro test: ', recall_micro_test)\n","print('Precision micro test: ', precision_micro_test)\n","\n","# Lưu dự đoán\n","result = pd.DataFrame([X_test, y_test_true, y_test_pred]).T\n","result.columns = ['text', 'true_label', 'pred_label']\n","path = '/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Predict CSV/ACD_Process'\n","result.to_csv(path + '/' + 'phoBert.csv', index=False)\n","result.head()\n","\n","#Lưu record\n","df = pd.DataFrame({\"Model\": [model_name[14]],\n","                   \"ACC_TRAIN\": [acc_train],\n","                   \"ACC_DEV\": [acc_dev],\n","                   \"ACC_TEST\": [acc_test],\n","                   \"PRECISION-MACRO\": [precision_macro_test],\n","                   \"RECALL-MACRO\":[recall_macro_test],\n","                    \"F1-MACRO\": [f1_macro_test],\n","                   \"PRECISION-MICRO\": [precision_micro_test],\n","                   \"RECALL-MICRO\":[recall_micro_test],\n","                   \"F1-MICRO\": [f1_micro_test]})\n","df.to_csv(\"/content/drive/MyDrive/NLP for Data Science/NLP_Project/Source Code/Evaluate/ACD_Process/ACD_Process.csv\",\n","          index = None, header = False, mode = \"a\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["https://huggingface.co/vinai/phobert-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf98m4u6s\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e71f6a2651ac4b8f814e613d899e398c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=557.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["storing https://huggingface.co/vinai/phobert-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n","creating metadata file for /root/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n","loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"transformers_version\": \"4.8.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0n4syzve\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b1a815a76b2442f98dcae52a0b90778","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=542923308.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["storing https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n","creating metadata file for /root/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n","loading weights file https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 258,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"PhobertTokenizer\",\n","  \"transformers_version\": \"4.8.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 64001\n","}\n","\n","https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp71b0dpvb\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"893c1d3ac1bd4056ab575cf2bb9fe4f8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=895321.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["storing https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n","creating metadata file for /root/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n","https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd08ztzer\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5495960939ef42c7ae914b713a9e7879","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1135173.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["storing https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes in cache at /root/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n","creating metadata file for /root/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n","loading file https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n","loading file https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes from cache at /root/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n","loading file https://huggingface.co/vinai/phobert-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/vinai/phobert-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer_config.json from cache at None\n","loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer.json from cache at None\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Adding <mask> to the vocabulary\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running training *****\n","  Num examples = 7028\n","  Num Epochs = 7\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3080\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3080' max='3080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3080/3080 09:48, Epoch 7/7]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.349600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.145500</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.098300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.071100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.050800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.039400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","***** Running Prediction *****\n","  Num examples = 7028\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='611' max='440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [440/440 00:27]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["***** Running Prediction *****\n","  Num examples = 771\n","  Batch size = 16\n","***** Running Prediction *****\n","  Num examples = 1938\n","  Batch size = 16\n"],"name":"stderr"},{"output_type":"stream","text":["=================================================================================\n","Accuracy train:  0.9395276038702334\n","Accuracy dev:  0.6731517509727627\n","Accuracy test:  0.6769865841073271\n","F1 macro test:  0.8150518739884016\n","Recall macro test:  0.795678763273961\n","Precision macro test:  0.8371954518734382\n","F1 micro test:  0.8225461613216715\n","Recall micro test:  0.804868771395968\n","Precision micro test:  0.8410174880763116\n"],"name":"stdout"}]}]}